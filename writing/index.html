<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Notes about my hobbies and other things I find interesting."><meta name=author content="Prabha Arivalagan"><link href=https://prabha.github.io/writing/ rel=canonical><link rel=alternate type=application/rss+xml title="RSS feed" href=../feed_rss_created.xml><link rel=alternate type=application/rss+xml title="RSS feed of updated content" href=../feed_rss_updated.xml><link rel=icon href=../logo.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.0"><title>Index - prabha.ai</title><link rel=stylesheet href=../assets/stylesheets/main.618322db.min.css><link rel=stylesheet href=../assets/stylesheets/palette.ab4e12ef.min.css><style>:root{--md-admonition-icon--note:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M1%207.775V2.75C1%201.784%201.784%201%202.75%201h5.025c.464%200%20.91.184%201.238.513l6.25%206.25a1.75%201.75%200%200%201%200%202.474l-5.026%205.026a1.75%201.75%200%200%201-2.474%200l-6.25-6.25A1.75%201.75%200%200%201%201%207.775m1.5%200c0%20.066.026.13.073.177l6.25%206.25a.25.25%200%200%200%20.354%200l5.025-5.025a.25.25%200%200%200%200-.354l-6.25-6.25a.25.25%200%200%200-.177-.073H2.75a.25.25%200%200%200-.25.25ZM6%205a1%201%200%201%201%200%202%201%201%200%200%201%200-2%22/%3E%3C/svg%3E');--md-admonition-icon--abstract:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2.5%201.75v11.5c0%20.138.112.25.25.25h3.17a.75.75%200%200%201%200%201.5H2.75A1.75%201.75%200%200%201%201%2013.25V1.75C1%20.784%201.784%200%202.75%200h8.5C12.216%200%2013%20.784%2013%201.75v7.736a.75.75%200%200%201-1.5%200V1.75a.25.25%200%200%200-.25-.25h-8.5a.25.25%200%200%200-.25.25m13.274%209.537zl-4.557%204.45a.75.75%200%200%201-1.055-.008l-1.943-1.95a.75.75%200%200%201%201.062-1.058l1.419%201.425%204.026-3.932a.75.75%200%201%201%201.048%201.074M4.75%204h4.5a.75.75%200%200%201%200%201.5h-4.5a.75.75%200%200%201%200-1.5M4%207.75A.75.75%200%200%201%204.75%207h2a.75.75%200%200%201%200%201.5h-2A.75.75%200%200%201%204%207.75%22/%3E%3C/svg%3E');--md-admonition-icon--info:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M0%208a8%208%200%201%201%2016%200A8%208%200%200%201%200%208m8-6.5a6.5%206.5%200%201%200%200%2013%206.5%206.5%200%200%200%200-13M6.5%207.75A.75.75%200%200%201%207.25%207h1a.75.75%200%200%201%20.75.75v2.75h.25a.75.75%200%200%201%200%201.5h-2a.75.75%200%200%201%200-1.5h.25v-2h-.25a.75.75%200%200%201-.75-.75M8%206a1%201%200%201%201%200-2%201%201%200%200%201%200%202%22/%3E%3C/svg%3E');--md-admonition-icon--tip:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M3.499.75a.75.75%200%200%201%201.5%200v.996C5.9%202.903%206.793%203.65%207.662%204.376l.24.202c-.036-.694.055-1.422.426-2.163C9.1.873%2010.794-.045%2012.622.26%2014.408.558%2016%201.94%2016%204.25c0%201.278-.954%202.575-2.44%202.734l.146.508.065.22c.203.701.412%201.455.476%202.226.142%201.707-.4%203.03-1.487%203.898C11.714%2014.671%2010.27%2015%208.75%2015h-6a.75.75%200%200%201%200-1.5h1.376a4.5%204.5%200%200%201-.563-1.191%203.84%203.84%200%200%201-.05-2.063%204.65%204.65%200%200%201-2.025-.293.75.75%200%200%201%20.525-1.406c1.357.507%202.376-.006%202.698-.318l.009-.01a.747.747%200%200%201%201.06%200%20.75.75%200%200%201-.012%201.074c-.912.92-.992%201.835-.768%202.586.221.74.745%201.337%201.196%201.621H8.75c1.343%200%202.398-.296%203.074-.836.635-.507%201.036-1.31.928-2.602-.05-.603-.216-1.224-.422-1.93l-.064-.221c-.12-.407-.246-.84-.353-1.29a2.4%202.4%200%200%201-.507-.441%203.1%203.1%200%200%201-.633-1.248.75.75%200%200%201%201.455-.364c.046.185.144.436.31.627.146.168.353.305.712.305.738%200%201.25-.615%201.25-1.25%200-1.47-.95-2.315-2.123-2.51-1.172-.196-2.227.387-2.706%201.345-.46.92-.27%201.774.019%203.062l.042.19.01.05c.348.443.666.949.94%201.553a.75.75%200%201%201-1.365.62c-.553-1.217-1.32-1.94-2.3-2.768L6.7%205.527c-.814-.68-1.75-1.462-2.692-2.619a3.7%203.7%200%200%200-1.023.88c-.406.495-.663%201.036-.722%201.508.116.122.306.21.591.239.388.038.797-.06%201.032-.19a.75.75%200%200%201%20.728%201.31c-.515.287-1.23.439-1.906.373-.682-.067-1.473-.38-1.879-1.193L.75%205.677V5.5c0-.984.48-1.94%201.077-2.664.46-.559%201.05-1.055%201.673-1.353z%22/%3E%3C/svg%3E');--md-admonition-icon--success:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M13.78%204.22a.75.75%200%200%201%200%201.06l-7.25%207.25a.75.75%200%200%201-1.06%200L2.22%209.28a.75.75%200%200%201%20.018-1.042.75.75%200%200%201%201.042-.018L6%2010.94l6.72-6.72a.75.75%200%200%201%201.06%200%22/%3E%3C/svg%3E');--md-admonition-icon--question:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M0%208a8%208%200%201%201%2016%200A8%208%200%200%201%200%208m8-6.5a6.5%206.5%200%201%200%200%2013%206.5%206.5%200%200%200%200-13M6.92%206.085h.001a.749.749%200%201%201-1.342-.67c.169-.339.436-.701.849-.977C6.845%204.16%207.369%204%208%204a2.76%202.76%200%200%201%201.637.525c.503.377.863.965.863%201.725%200%20.448-.115.83-.329%201.15-.205.307-.47.513-.692.662-.109.072-.22.138-.313.195l-.006.004a6%206%200%200%200-.26.16%201%201%200%200%200-.276.245.75.75%200%200%201-1.248-.832c.184-.264.42-.489.692-.661q.154-.1.313-.195l.007-.004c.1-.061.182-.11.258-.161a1%201%200%200%200%20.277-.245C8.96%206.514%209%206.427%209%206.25a.61.61%200%200%200-.262-.525A1.27%201.27%200%200%200%208%205.5c-.369%200-.595.09-.74.187a1%201%200%200%200-.34.398M9%2011a1%201%200%201%201-2%200%201%201%200%200%201%202%200%22/%3E%3C/svg%3E');--md-admonition-icon--warning:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M6.457%201.047c.659-1.234%202.427-1.234%203.086%200l6.082%2011.378A1.75%201.75%200%200%201%2014.082%2015H1.918a1.75%201.75%200%200%201-1.543-2.575Zm1.763.707a.25.25%200%200%200-.44%200L1.698%2013.132a.25.25%200%200%200%20.22.368h12.164a.25.25%200%200%200%20.22-.368Zm.53%203.996v2.5a.75.75%200%200%201-1.5%200v-2.5a.75.75%200%200%201%201.5%200M9%2011a1%201%200%201%201-2%200%201%201%200%200%201%202%200%22/%3E%3C/svg%3E');--md-admonition-icon--failure:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2.344%202.343za8%208%200%200%201%2011.314%2011.314A8.002%208.002%200%200%201%20.234%2010.089a8%208%200%200%201%202.11-7.746m1.06%2010.253a6.5%206.5%200%201%200%209.108-9.275%206.5%206.5%200%200%200-9.108%209.275M6.03%204.97%208%206.94l1.97-1.97a.749.749%200%200%201%201.275.326.75.75%200%200%201-.215.734L9.06%208l1.97%201.97a.749.749%200%200%201-.326%201.275.75.75%200%200%201-.734-.215L8%209.06l-1.97%201.97a.749.749%200%200%201-1.275-.326.75.75%200%200%201%20.215-.734L6.94%208%204.97%206.03a.75.75%200%200%201%20.018-1.042.75.75%200%200%201%201.042-.018%22/%3E%3C/svg%3E');--md-admonition-icon--danger:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M9.504.43a1.516%201.516%200%200%201%202.437%201.713L10.415%205.5h2.123c1.57%200%202.346%201.909%201.22%203.004l-7.34%207.142a1.25%201.25%200%200%201-.871.354h-.302a1.25%201.25%200%200%201-1.157-1.723L5.633%2010.5H3.462c-1.57%200-2.346-1.909-1.22-3.004zm1.047%201.074L3.286%208.571A.25.25%200%200%200%203.462%209H6.75a.75.75%200%200%201%20.694%201.034l-1.713%204.188%206.982-6.793A.25.25%200%200%200%2012.538%207H9.25a.75.75%200%200%201-.683-1.06l2.008-4.418.003-.006-.004-.009-.006-.006-.008-.001q-.005%200-.009.004%22/%3E%3C/svg%3E');--md-admonition-icon--bug:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M4.72.22a.75.75%200%200%201%201.06%200l1%20.999a3.5%203.5%200%200%201%202.441%200l.999-1a.748.748%200%200%201%201.265.332.75.75%200%200%201-.205.729l-.775.776c.616.63.995%201.493.995%202.444v.327q0%20.15-.025.292c.408.14.764.392%201.029.722l1.968-.787a.75.75%200%200%201%20.556%201.392L13%207.258V9h2.25a.75.75%200%200%201%200%201.5H13v.5q-.002.615-.141%201.186l2.17.868a.75.75%200%200%201-.557%201.392l-2.184-.873A5%205%200%200%201%208%2016a5%205%200%200%201-4.288-2.427l-2.183.873a.75.75%200%200%201-.558-1.392l2.17-.868A5%205%200%200%201%203%2011v-.5H.75a.75.75%200%200%201%200-1.5H3V7.258L.971%206.446a.75.75%200%200%201%20.558-1.392l1.967.787c.265-.33.62-.583%201.03-.722a1.7%201.7%200%200%201-.026-.292V4.5c0-.951.38-1.814.995-2.444L4.72%201.28a.75.75%200%200%201%200-1.06m.53%206.28a.75.75%200%200%200-.75.75V11a3.5%203.5%200%201%200%207%200V7.25a.75.75%200%200%200-.75-.75ZM6.173%205h3.654A.17.17%200%200%200%2010%204.827V4.5a2%202%200%201%200-4%200v.327c0%20.096.077.173.173.173%22/%3E%3C/svg%3E');--md-admonition-icon--example:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5%205.782V2.5h-.25a.75.75%200%200%201%200-1.5h6.5a.75.75%200%200%201%200%201.5H11v3.282l3.666%205.76C15.619%2013.04%2014.543%2015%2012.767%2015H3.233c-1.776%200-2.852-1.96-1.899-3.458Zm-2.4%206.565a.75.75%200%200%200%20.633%201.153h9.534a.75.75%200%200%200%20.633-1.153L12.225%2010.5h-8.45ZM9.5%202.5h-3V6c0%20.143-.04.283-.117.403L4.73%209h6.54L9.617%206.403A.75.75%200%200%201%209.5%206Z%22/%3E%3C/svg%3E');--md-admonition-icon--quote:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M1.75%202.5h10.5a.75.75%200%200%201%200%201.5H1.75a.75.75%200%200%201%200-1.5m4%205h8.5a.75.75%200%200%201%200%201.5h-8.5a.75.75%200%200%201%200-1.5m0%205h8.5a.75.75%200%200%201%200%201.5h-8.5a.75.75%200%200%201%200-1.5M2.5%207.75v6a.75.75%200%200%201-1.5%200v-6a.75.75%200%200%201%201.5%200%22/%3E%3C/svg%3E');}</style><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Courier+Prime:300,300i,400,400i,700,700i%7CCourier+Prime:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Courier Prime";--md-code-font:"Courier Prime"}</style><link rel=stylesheet href=../assets/_mkdocstrings.css><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><meta property=og:type content=website><meta property=og:title content="Index - prabha.ai"><meta property=og:description content="Notes about my hobbies and other things I find interesting."><meta property=og:image content=https://prabha.github.io/assets/images/social/writing/index.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta content=https://prabha.github.io/writing/ property=og:url><meta property=twitter:card content=summary_large_image><meta property=twitter:title content="Index - prabha.ai"><meta property=twitter:description content="Notes about my hobbies and other things I find interesting."><meta property=twitter:image content=https://prabha.github.io/assets/images/social/writing/index.png></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title=prabha.ai class="md-header__button md-logo" aria-label=prabha.ai data-md-component=logo> <img src=../logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> prabha.ai </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Index </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/prabha-git/prabha-git.github.io title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> blog </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=./ class=md-tabs__link> Blog </a> </li> <li class=md-tabs__item> <a href=../contact/ class=md-tabs__link> Contact </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title=prabha.ai class="md-nav__button md-logo" aria-label=prabha.ai data-md-component=logo> <img src=../logo.png alt=logo> </a> prabha.ai </label> <div class=md-nav__source> <a href=https://github.com/prabha-git/prabha-git.github.io title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> blog </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=./ class=md-nav__link> <span class=md-ellipsis> Blog </span> </a> </li> <li class=md-nav__item> <a href=../contact/ class=md-nav__link> <span class=md-ellipsis> Contact </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <div class=md-content__inner> <header class=md-typeset> </header> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=https://avatars.githubusercontent.com/u/3776681 alt="Prabha Arivalagan"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2025-12-05 00:00:00+00:00">2025/12/05</time></li> <li class=md-meta__item> 3 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=the-evolution-of-ai-assisted-development-from-developer-to-manager><a href=2025/12/05/evolution-ai-coding/ class=toclink>The Evolution of AI-Assisted Development: From Developer to Manager</a></h2> <p><em>Last Updated: December 2025</em></p> <p>The developer's relationship with code is fundamentally changing. As AI becomes more capable, the developer's role transforms with it. This isn't about better tools; it's about a shift in what developers actually <em>do</em>.</p> <p><img alt="The evolution of AI-assisted development: from crawl to fly" src=img/ai-evolution-crawl-walk-run-fly.png></p> <hr> <h3 id=the-baseline-the-developer><a class=toclink href=2025/12/05/evolution-ai-coding/#the-baseline-the-developer>The Baseline: The Developer</a></h3> <p>Before AI entered the picture, developers wrote every line of code. The IDE offered syntax highlighting and basic autocomplete, but these were rule-based, not learned. The code came from your fingers, your knowledge, your reasoning.</p> <p><strong>The Developer:</strong> You owned the code completely. Problem decomposition, algorithmic thinking, debugging through logical reasoning. Every line was intentional. Every bug was yours to find and fix.</p> <p><strong>The AI:</strong> None.</p> <p>This was slow, mentally taxing, but you understood everything you shipped.</p> <hr> <h3 id=ai-learned-to-predict-and-developers-got-faster><a class=toclink href=2025/12/05/evolution-ai-coding/#ai-learned-to-predict-and-developers-got-faster>AI Learned to Predict, and Developers Got Faster</a></h3> <p>AI began recognizing patterns in code. It could predict the next few characters, the next line, sometimes a whole block. Tab-tab-tab through boilerplate.</p> <p><strong>The Developer:</strong> You still wrote the code. AI just made your fingers faster. But now you needed a new skill: spotting when a suggestion was subtly wrong. You had to maintain your mental model while accepting code you didn't type.</p> <p><strong>The AI:</strong> Finishes your sentences. Suggests what comes next based on patterns it has seen.</p> <p>Speed increased, but something shifted. You started accepting code without fully reading it. You stopped memorizing syntax; why bother when Tab provides it?</p> <hr> <h3 id=ai-learned-intent-and-developers-became-architects><a class=toclink href=2025/12/05/evolution-ai-coding/#ai-learned-intent-and-developers-became-architects>AI Learned Intent, and Developers Became Architects</a></h3> <p>Language models grew larger. AI could now understand what you meant, not just what you typed. "Write a function that validates email addresses" became reality in seconds. You could describe the <em>what</em> and AI would handle the <em>how</em>.</p> <p><strong>The Developer:</strong> You became an architect. You directed the work, specified at the function level, and wove individual outputs into a coherent system. AI was your hands; you remained the brain. You still reviewed each piece, still understood the code you shipped, even if you didn't type it.</p> <p><strong>The AI:</strong> Takes instructions and builds functions or code blocks. Explains unfamiliar code. Answers questions about your codebase.</p> <p>Productivity exploded. But a new risk emerged: "vibe coding," accepting code that works without understanding why. The gap between what you could do with AI and without AI widened.</p> <p>Geoffrey Litt calls this the <a href=https://www.geoffreylitt.com/2025/10/24/code-like-a-surgeon>surgeon model</a>: stay hands-on for primary work, delegate secondary tasks to AI. You're not managing AI; you're operating with AI support.</p> <hr> <h3 id=ai-executes-autonomously-and-developers-become-managers-emerging><a class=toclink href=2025/12/05/evolution-ai-coding/#ai-executes-autonomously-and-developers-become-managers-emerging>AI Executes Autonomously, and Developers Become Managers (Emerging)</a></h3> <p>AI is learning to reason, plan, and execute multi-step tasks without hand-holding. Assign a bug, and the agent investigates, edits files, runs tests, and opens a pull request. You review the deliverable, not the process.</p> <p><strong>The Developer:</strong> You become a manager. You assign work, define outcomes, set acceptance criteria. You orchestrate multiple workstreams toward the product goal. The code is no longer yours; the outcome is. You review at the deliverable level, not line-by-line.</p> <p><strong>The AI:</strong> Works independently on assigned tasks. Plans, codes, tests, and delivers.</p> <p>This phase is emerging, not arrived. Google just released <a href=https://antigravity.google>Antigravity</a> last week. Claude Code, Cursor, and GitHub Copilot all have agentic capabilities now. These tools are promising for prototypes and personal projects, but not yet enterprise production ready. Trust takes time to build.</p> <p>And let's be honest: even the smartest models make basic mistakes. Building a production system with just prompting remains far-fetched. Code written entirely by AI tends to be bloated, unoptimized, and hard to maintain.</p> <p>As <a class="magiclink magiclink-github magiclink-mention" href=https://github.com/catalinmpit title="GitHub User: catalinmpit">@catalinmpit</a> put it: "Vibe coding is easy. Vibe debugging is the hard part."</p> <hr> <h3 id=the-transformation-continues><a class=toclink href=2025/12/05/evolution-ai-coding/#the-transformation-continues>The Transformation Continues</a></h3> <p>Each shift follows the same pattern: AI becomes capable of something new, and the developer's role moves up a level of abstraction.</p> <ul> <li><strong>Autocomplete:</strong> AI predicts tokens → Developer still writes, but faster</li> <li><strong>Copilot/Chat:</strong> AI understands intent → Developer directs, AI executes</li> <li><strong>Agents:</strong> AI reasons autonomously → Developer manages, AI delivers</li> </ul> <p>The constant is change. The role keeps shifting. What worked yesterday may not be enough tomorrow.</p> <p>The question isn't which phase is "best." It's recognizing where you are, understanding where things are heading, and adapting as the transformation continues.</p> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=https://avatars.githubusercontent.com/u/3776681 alt="Prabha Arivalagan"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2025-10-04 00:00:00+00:00">2025/10/04</time></li> <li class=md-meta__item> 6 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=building-ai-agents-in-google-cloud-choose-the-right-approach-for-your-needs><a href=2025/10/04/agents-in-gcp/ class=toclink>Building AI Agents in Google Cloud: Choose the Right Approach for Your Needs</a></h2> <h3 id=tldr><a class=toclink href=2025/10/04/agents-in-gcp/#tldr>TL;DR</a></h3> <ul> <li><strong>ADK</strong> → Google-developed open-source framework for building complex multi-agent systems with maximum control and modularity</li> <li><strong>Conversational Agents (Dialogflow CX)</strong> → Omnichannel customer conversations with structured flows and open-ended playbooks</li> <li><strong>Open-Source Frameworks</strong> → Leverage specific framework capabilities (LangChain's integrations, LangGraph's state management, CrewAI's collaboration) on GCP's managed infrastructure</li> <li><strong>Agentspace</strong> → Enterprise search platform and self-serve agent creation for automating everyday knowledge work tasks </li> </ul> <h3 id=key-decision-point-choose-your-building-framework-adkconversational-agentsopen-source-then-optionally-deploy-to-agentspace-for-enterprise-wide-access><a class=toclink href=2025/10/04/agents-in-gcp/#key-decision-point-choose-your-building-framework-adkconversational-agentsopen-source-then-optionally-deploy-to-agentspace-for-enterprise-wide-access><strong>Key decision point:</strong> Choose your building framework (ADK/Conversational Agents/Open-Source), then optionally deploy to Agentspace for enterprise wide access.</a></h3> <p>Building AI agents in Google Cloud Platform presents four distinct paths, each optimized for different use cases. The challenge isn't finding options—it's choosing the right one.</p> <p>This guide cuts through the complexity to help you make the right decision based on your specific business needs, not technical preferences.</p> <h3 id=decision-framework><a class=toclink href=2025/10/04/agents-in-gcp/#decision-framework>Decision Framework</a></h3> <pre class=mermaid><code>flowchart TD
    Start{Type of Agent?} --&gt; Path{Purpose?}
    Path --&gt;|Conversational&lt;br/&gt;Chat &amp; Voice| MultiTurn{Need multi-turn&lt;br/&gt;dialogues across&lt;br/&gt;channels?}
    MultiTurn --&gt;|Yes| DialogFlow[Conversational Agents&lt;br/&gt;Dialogflow CX]
    MultiTurn --&gt;|No| ADKCheck

    Path --&gt;|Agentic&lt;br/&gt;Workflows| ADKCheck{Building multi-agent&lt;br/&gt;systems with native&lt;br/&gt;GCP ecosystem?}
    ADKCheck --&gt;|Yes| ADK[Build with ADK]
    ADKCheck --&gt;|No| Framework{Want specific&lt;br/&gt;framework features?&lt;br/&gt;LangChain/LangGraph/&lt;br/&gt;CrewAI/AG2}
    Framework --&gt;|Yes| OpenSource[Build with&lt;br/&gt;LangChain/LangGraph/&lt;br/&gt;CrewAI/AG2]
    Framework --&gt;|No| ADK[Build with ADK]

    ADK --&gt; AgentEngine[Agent Engine&lt;br/&gt;Deployment]
    OpenSource --&gt; AgentEngine</code></pre> <p>*Note: Agents deployed via Agent Engine and Conversational Agents can be integrated into Agentspace for organization-wide access</p> <h3 id=four-approaches-to-building-ai-agents-in-gcp><a class=toclink href=2025/10/04/agents-in-gcp/#four-approaches-to-building-ai-agents-in-gcp>Four Approaches to Building AI Agents in GCP</a></h3> <h4 id=1-agent-development-kit-adk-for-complex-multi-agent-orchestration><a class=toclink href=2025/10/04/agents-in-gcp/#1-agent-development-kit-adk-for-complex-multi-agent-orchestration>1. Agent Development Kit (ADK) - For Complex Multi-Agent Orchestration</a></h4> <p><strong>Best for:</strong> Multi-agent workflows requiring complex routing and orchestration with programmatic control</p> <p>ADK is Google's open-source framework for building sophisticated multi-agent systems with complete programmatic control.</p> <p><strong>When to choose ADK:</strong> - You need sophisticated multi-agent orchestration with hierarchical delegation - You need to orchestrate complex integrations with conditional logic and custom error handling across enterprise systems - Your use case involves backend automation or event-driven workflows (scheduled jobs, API-triggered processes, continuous monitoring) without user-initiated conversations</p> <p><strong>Why ADK excels at this:</strong> - Native GCP integration with built-in tools for BigQuery, AlloyDB, Cloud SQL, and direct access to Vertex AI services - Multimodal capabilities with documents, audio, and video, plus bidirectional streaming for real-time voice interactions - Modular architecture for independent agent development - 100+ enterprise connectors via Application Integration Toolset - Built-in evaluation framework (web UI, pytest, CLI) with CI/CD pipeline integration, step-by-step debugging with trace inspection, and comprehensive audit logging</p> <p><strong>Not the right fit when:</strong> - A simpler solution suffices - if you need a basic RAG bot or simple workflow, ADK may be overkill</p> <p><strong>Example Business use cases where ADK excels:</strong> - <strong>Financial reporting</strong> - Multi-source data aggregation, validation, and quarterly report generation triggered by schedule or events - <strong>Insurance claim processing</strong> - End-to-end claim automation with document extraction, policy validation, fraud detection, and intelligent routing - <strong>Mortgage underwriting</strong> - Document extraction agent, income verification agent, credit risk agent, and compliance agent coordinate with dynamic routing based on loan type, applicant profile, and regulatory requirements - <strong>Incident response</strong> - Real-time log analysis with root cause identification, automated rollbacks, and post-mortem generation</p> <h4 id=2-conversational-agents-dialogflow-cx-for-multi-turn-conversation><a class=toclink href=2025/10/04/agents-in-gcp/#2-conversational-agents-dialogflow-cx-for-multi-turn-conversation>2. Conversational Agents (Dialogflow CX) - For Multi-turn conversation</a></h4> <p><strong>Best for:</strong> Interactive user conversations requiring multi-turn dialogues across multiple channels chat, voice etc </p> <p>Conversational Agents (Dialogflow CX) combines structured <strong>flows</strong> and open-ended <strong>playbooks</strong> for building customer service experiences across all channels.</p> <p><strong>When to choose Conversational Agents:</strong> - Your use case involves <strong>conversational interactions with users</strong> (customer support, sales, ordering, troubleshooting, IT helpdesk, etc.) - Users initiate conversations and expect natural, multi-turn dialogues - You need to serve conversations across multiple touchpoints (web chat, mobile app, phone/voice, contact centers) - Your conversations require both structured flows (predictable paths like order status checks) AND open-ended dialogues (general questions using knowledge bases)</p> <p><strong>Why Conversational Agents excels at this:</strong> - Flows (structured) and playbooks (open-ended) conversation patterns - Native omnichannel deployment with 30+ language support - Visual flow builders for non-technical users - Deep contact center integration</p> <p><strong>Not the right fit when:</strong> - Primary task is not conversational - better tools exist for backend automation or data processing - No multi-turn conversation flows - simpler solutions may suffice for one-off Q&amp;A</p> <p><strong>Business use cases where Conversational Agents excel:</strong> - <strong>Customer support</strong> - Multi-turn conversations for order status, returns, and product questions across web, mobile, and phone - <strong>Voice IVR</strong> - Natural language phone interactions for account management with 30+ language support and telephony integration - <strong>Restaurant ordering</strong> - Conversational order-taking with menu navigation, customization, and payment collection - <strong>Technical support</strong> - Guided troubleshooting with state-managed diagnostic workflows and human handoff capabilities</p> <h4 id=3-open-source-frameworks-on-vertex-ai-for-framework-specific-capabilities><a class=toclink href=2025/10/04/agents-in-gcp/#3-open-source-frameworks-on-vertex-ai-for-framework-specific-capabilities>3. Open-Source Frameworks on Vertex AI - For Framework-Specific Capabilities</a></h4> <p><strong>Best for:</strong> Teams wanting specific framework features (LangChain's integrations, LangGraph's state management, CrewAI's collaboration, AG2's multi-agent patterns, LlamaIndex's RAG pipelines) on GCP's managed infrastructure</p> <p>Vertex AI Agent Engine deploys your existing open-source framework code (LangChain, LangGraph, CrewAI, AG2, LlamaIndex) with managed infrastructure and deep GCP integrations.</p> <p><strong>When to choose this path:</strong> - You need specific framework capabilities (LangChain's 700+ integrations, LangGraph's state machines, CrewAI's role-based collaboration, AG2's multi-agent debates, LlamaIndex's advanced RAG) - You've already invested in framework codebases and want to deploy to production without rebuilding - You want rapid prototyping with framework-specific features (vector stores, document loaders, APIs, state management patterns) - You need GCP's managed infrastructure without abandoning framework expertise</p> <p><strong>Why this excels:</strong> - Leverage open-source framework capabilities on GCP's enterprise-grade managed infrastructure - Managed infrastructure with autoscaling - Enterprise-grade security and compliance built-in - Direct access to GCP services (BigQuery, Cloud SQL, etc.) - Dedicated SDK support for major frameworks - Access to Vertex AI Gen AI Evaluation service for agent performance assessment and trajectory analysis</p> <p><strong>Not the right fit when:</strong> - You don't need framework-specific features - ADK or Conversational Agents provide simpler, more streamlined solutions for GCP</p> <p><strong>Business scenarios where you should use open-source frameworks:</strong> - <strong>RAG support bot</strong> - Existing LangChain/LlamaIndex Q&amp;A system with custom retrieval ready for production scaling - <strong>Document analysis</strong> - LlamaIndex investment with custom indices and query pipelines for contract analysis - <strong>Research assistant</strong> - LangGraph application with complex state management and human-in-the-loop workflows</p> <h4 id=4-google-agentspace-enterprise-search-agent-platform><a class=toclink href=2025/10/04/agents-in-gcp/#4-google-agentspace-enterprise-search-agent-platform>4. Google Agentspace - Enterprise Search &amp; Agent Platform</a></h4> <p><strong>Best for:</strong> Organizations needing enterprise-wide search across data sources and self-serve agent creation for everyday knowledge work tasks</p> <p>Agentspace provides enterprise search across 100+ data sources and serves as a central hub for organizational agents. Enterprise Plus edition includes Agent Designer for creating no-code agents that automate everyday knowledge work.</p> <p><strong>Key capabilities:</strong> - Unified enterprise search across 100+ data sources (Microsoft SharePoint, Confluence, ServiceNow, Google Drive, etc.) - Self-serve Agent Designer for creating agents that automate everyday tasks (emails, scheduling, admin work) - Enterprise Plus edition - Central deployment hub ("From your company") for organizational agents from ADK, Dialogflow CX, and partner sources - Three editions: Enterprise, Enterprise Plus (includes Agent Designer), and Frontline</p> <p><strong>When to choose Agentspace:</strong> - You need enterprise-wide access to search and agents that respect individual user permissions and data access controls - Users want to automate simple day-to-day tasks (scheduling, emails, admin coordination) with permission-aware agents (requires Enterprise Plus for Agent Designer) - You need a central hub for deploying organizational agents from ADK (via Agent Engine), Dialogflow CX, and partner sources across the enterprise</p> <p><strong>Not the right fit when:</strong> - Your agents serve external customers rather than internal employees - Use Conversational Agents or ADK with direct deployment - You need complex orchestration or advanced agent capabilities - Build with ADK, Dialogflow CX, or open-source frameworks first (can deploy to Agentspace later) - You don't need enterprise-wide agent discovery or user-permission-aware search - Deploy agents directly via Agent Engine or Dialogflow CX - Budget constraints with per-seat licensing model - Pay-per-use options (ADK, Vertex AI Agent Engine) may be more economical</p> <h3 id=side-by-side-comparison><a class=toclink href=2025/10/04/agents-in-gcp/#side-by-side-comparison>Side-by-Side Comparison</a></h3> <table> <thead> <tr> <th>Aspect</th> <th>ADK (Google's Open-Source)</th> <th>Open-Source Frameworks</th> <th>Conversational Agents</th> <th>Agentspace</th> </tr> </thead> <tbody> <tr> <td><strong>Best For</strong></td> <td>Build multi-agent systems</td> <td>Build multi-agent systems</td> <td>Interactive user conversations</td> <td>Permission-aware enterprise search &amp; agents</td> </tr> <tr> <td><strong>Development Approach</strong></td> <td>Code-first</td> <td>Code-first</td> <td>Visual flow builder + Natural Language</td> <td>Agent Designer (Enterprise Plus)</td> </tr> <tr> <td><strong>Key Strength</strong></td> <td>Native GCP integration + multimodal</td> <td>Based on Framework</td> <td>Omnichannel support</td> <td>User-permission-aware access</td> </tr> <tr> <td><strong>Hosting Platform</strong></td> <td>Vertex AI Agent Engine</td> <td>Vertex AI Agent Engine</td> <td>Google-managed service</td> <td>Google-managed service</td> </tr> </tbody> </table> <h3 id=summary><a class=toclink href=2025/10/04/agents-in-gcp/#summary>Summary</a></h3> <p>Building AI agents in GCP comes down to choosing the right tool for your specific needs: ADK for multi-agent orchestration with native GCP integration, Conversational Agents for interactive dialogues across channels, Community Frameworks for leveraging specific framework capabilities, or Agentspace for enterprise-wide permission-aware search and agents.</p> <p>All four approaches are production-ready and can work together—many organizations use multiple approaches for different use cases. Start with your use case, validate with a proof-of-concept, and focus on business value over technical complexity.</p> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=https://avatars.githubusercontent.com/u/3776681 alt="Prabha Arivalagan"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2025-05-13 00:00:00+00:00">2025/05/13</time></li> <li class=md-meta__item> 6 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=the-language-of-agents-decoding-messages-in-langchain-langgraph><a href=2025/05/13/the-language-of-agents/ class=toclink>The Language of Agents Decoding Messages in LangChain &amp; LangGraph</a></h2> <p>Ever wondered how apps get AI to chat, follow instructions, or even use tools? A lot of the magic comes down to "messages." Think of them as the notes passed between you, the AI, and any other services involved. LangChain and LangGraph are awesome tools that help manage these messages, making it easier to build cool AI-powered stuff. Let's break down how it works, keeping it simple!</p> <h3 id=the-main-players-core-message-types><a class=toclink href=2025/05/13/the-language-of-agents/#the-main-players-core-message-types>The Main Players: Core Message Types</a></h3> <p>LangChain uses a few key message types to keep conversations organized. These are the building blocks for almost any chat interaction.</p> <h4 id=systemmessage-setting-the-scene><a class=toclink href=2025/05/13/the-language-of-agents/#systemmessage-setting-the-scene>SystemMessage: Setting the Scene</a></h4> <p>This message sets the stage. It tells the AI how to behave – its personality, its job, or any ground rules. Think of it as whispering to the AI, "You're a super helpful assistant who loves pirate jokes." You usually send this one first. LangChain figures out how to pass this instruction to different AI models, even if they have their own quirks for system prompts.</p> <h4 id=humanmessage-what-you-say><a class=toclink href=2025/05/13/the-language-of-agents/#humanmessage-what-you-say>HumanMessage: What You Say</a></h4> <p>Simple enough – this is your input. When you ask a question or give a command, LangChain wraps it up as a HumanMessage. It can be plain text or even include images if the AI supports it. If you just send a string to a chat model, LangChain often handily turns it into a HumanMessage for you.</p> <h4 id=aimessage-the-ais-response><a class=toclink href=2025/05/13/the-language-of-agents/#aimessage-the-ais-response>AIMessage: The AI's Response</a></h4> <p>This is what the AI says back. It's not just text, though! An AIMessage can also include requests for the AI to use "tools" (like searching the web or running some code) and other useful bits like how many tokens it used. If the AI is streaming its response, you'll see AIMessageChunks that build up the full reply.</p> <h4 id=toolmessage-reporting-back-from-a-mission><a class=toclink href=2025/05/13/the-language-of-agents/#toolmessage-reporting-back-from-a-mission>ToolMessage: Reporting Back from a Mission</a></h4> <p>If the AI (via an AIMessage) asks to use a tool, your app will run that tool and then send the results back using a ToolMessage. This message needs a special <code>tool_call_id</code> to link it to the AI's original request, which is super important if the AI wants to use multiple tools at once. This is the modern way, an upgrade from the older FunctionMessage.</p> <h3 id=going-off-script-chatmessage-and-custom-roles><a class=toclink href=2025/05/13/the-language-of-agents/#going-off-script-chatmessage-and-custom-roles>Going Off-Script: ChatMessage and Custom Roles</a></h3> <p>What if you need a role that's not "system," "user," "assistant," or "tool"? LangChain offers <code>ChatMessage</code> for that. It lets you set any role label you want.</p> <p>But here's the catch: most big AI models (like OpenAI's GPTs) only understand the standard roles. If you send a <code>ChatMessage</code> with a role like "developer_instructions," they'll likely ignore it or throw an error. So, only use <code>ChatMessage</code> with custom roles if you know your specific AI model supports them. For example, some Ollama models use a "control" role for special commands, and <code>ChatMessage</code> is how you'd send that.</p> <h3 id=who-said-that-the-name-attribute><a class=toclink href=2025/05/13/the-language-of-agents/#who-said-that-the-name-attribute>Who Said That? The <code>name</code> Attribute</a></h3> <p>In a busy chat with multiple users or AI agents, how do you know who said what? All LangChain message classes (HumanMessage, AIMessage, etc.) have an optional <code>name</code> attribute. Its job is to distinguish between different speakers who might share the same role – for instance, to tell "Alice's" HumanMessage from "Bob's."</p> <p>Provider support for this <code>name</code> field varies. OpenAI’s Chat API (and therefore Azure OpenAI) allows you to set a <code>name</code> on user or assistant messages, so the model can keep track of different participants. However, many other models might ignore or drop the <code>name</code>; LangChain will usually just omit it when sending the request to such models.</p> <p>In multi-agent setups with LangGraph, this <code>name</code> field is super handy for tagging which agent sent a message, like <code>AIMessage(content="Here’s what I found.", name="ResearchBot")</code>. Even if the underlying AI model doesn't use the <code>name</code>, it's still useful metadata for your application's logic.</p> <h3 id=team-chat-messages-in-langgraph-multi-agent-systems><a class=toclink href=2025/05/13/the-language-of-agents/#team-chat-messages-in-langgraph-multi-agent-systems>Team Chat: Messages in LangGraph Multi-Agent Systems</a></h3> <p>LangGraph helps you build apps where multiple AI agents work together. Their coordination hinges on a shared message history, typically managed within a component like <code>MessagesState</code>. This acts as a central ledger of the conversation.</p> <p>Think of it like a meticulously recorded group project chat where everyone sees all messages in the order they were sent. When it's an agent's turn to contribute:</p> <ol> <li> <p><strong>Reads the History:</strong> It first accesses the entire current chat history from <code>MessagesState</code>. This gives it full context of everything that has transpired across all participating agents.</p> </li> <li> <p><strong>Performs its Task:</strong> The agent then does its designated job, which might involve thinking, calling an LLM, or using a tool.</p> </li> <li> <p><strong>Writes Back:</strong> Finally, it appends its own messages (e.g., an <code>AIMessage</code> detailing its findings or actions) to the shared <code>MessagesState</code>, thus adding to the ongoing conversation.</p> </li> </ol> <p>This cycle ensures that if Agent A contributes, and then Agent B takes over, Agent B has visibility into the initial request and Agent A's input. This shared, incrementally built history is fundamental.</p> <p><strong>Maintaining Order and Clarity in Shared History:</strong></p> <p>For this system to work without confusion, two things are crucial: messages must be in the correct order, and it must be clear who (or what) sent each message.</p> <ul> <li> <p><strong>Chronological Order:</strong> <code>MessagesState</code> inherently maintains messages in the order they are added. Each new message is appended, preserving a chronological flow. Furthermore, LangGraph's graph structure itself dictates the sequence of which agent or tool operates next. This controlled execution ensures that contributions are added to the history in a predictable and understandable order.</p> </li> <li> <p><strong>Role and Sender Identification:</strong> Knowing "who said what" is vital. While the next section, "Tagging Agents in the Flow," dives deeper into specific techniques like the <code>name</code> attribute and agent-specific <code>SystemMessage</code>s, the core idea is that each message carries information about its origin and purpose. The message type itself (e.g., <code>HumanMessage</code>, <code>AIMessage</code>, <code>ToolMessage</code>) provides an initial layer of role definition.</p> </li> </ul> <p>LangGraph leverages this ordered and attributed message history to enable complex interactions. While you can design custom mechanisms to pass specific pieces of information between agents directly, the default and foundational approach is this transparent, shared history that all agents can access and contribute to sequentially.</p> <h4 id=tagging-agents-in-the-flow><a class=toclink href=2025/05/13/the-language-of-agents/#tagging-agents-in-the-flow>Tagging Agents in the Flow:</a></h4> <p>How do you know which agent said what in this shared history?</p> <ul> <li> <p><strong><code>name</code> attribute:</strong> As mentioned, <code>AIMessage(content="...", name="FlightBot")</code>.</p> </li> <li> <p><strong>System Prompts:</strong> Give each agent its own <code>SystemMessage</code> like, "You are HotelBot, specializing in booking accommodations."</p> </li> <li> <p><strong>LangGraph State:</strong> You can even add a field to your graph's state to track the <code>last_active_agent</code>.</p> </li> </ul> <p>The safest bet is often to give each agent a clear system instruction about its identity and also log which agent produced each message.</p> <h3 id=quick-tips-for-message-mastery><a class=toclink href=2025/05/13/the-language-of-agents/#quick-tips-for-message-mastery>Quick Tips for Message Mastery</a></h3> <ul> <li> <p><strong>Order Matters (Usually):</strong> A common pattern is <code>SystemMessage</code> first (if you need one), then a <code>HumanMessage</code> from the user, followed by an <code>AIMessage</code> from the model, and then back and forth between Human and AI messages. If tools are involved, <code>ToolMessage</code>s follow the <code>AIMessage</code> that requested the tool.</p> </li> <li> <p><strong>Full Context is King:</strong> Unless you're specifically managing memory (like trimming old messages), feed the AI the whole accumulated list of messages each time. This gives it the best context. In LangGraph, <code>MessagesState</code> often handles this by injecting the up-to-date history into each agent node.</p> </li> <li> <p><strong>LangChain Simplifies:</strong> If you pass a plain string to a chat model, LangChain usually wraps it in a <code>HumanMessage</code> for you.</p> </li> <li> <p><strong>Let LangChain Do the Heavy Lifting:</strong> Write your code with LangChain's standard messages. It'll handle the translation to whatever format the specific AI model needs. This makes your code cleaner and easier to switch between different AIs.</p> </li> <li> <p><strong>Mind the Memory:</strong> Long chat histories can get too big for an AI's context window (and cost more!). LangGraph and LangChain offer ways to trim or summarize old messages to keep things manageable.</p> </li> </ul> <h3 id=wrapping-up><a class=toclink href=2025/05/13/the-language-of-agents/#wrapping-up>Wrapping Up</a></h3> <p>Messages are the lifeblood of your LangChain and LangGraph applications. Understanding these basic types, how the <code>name</code> field helps with identity, and how messages flow in multi-agent systems will help you build more powerful and reliable AI tools. Stick to the standards when you can, use custom options wisely, and let LangChain handle the provider-specific details. By following these conventions, you'll ensure clear message sequences and effective agent collaboration. Happy building!</p> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=https://avatars.githubusercontent.com/u/3776681 alt="Prabha Arivalagan"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2024-08-18 00:00:00+00:00">2024/08/18</time></li> <li class=md-meta__item> 9 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=building-personal-chatbot-part-2><a href=2024/08/18/building-obsidian-kb-chatbot/ class=toclink>Building Personal Chatbot - Part 2</a></h2> <h2 id=enhancing-our-obsidian-chatbot-advanced-rag-techniques-with-langchain><a class=toclink href=2024/08/18/building-obsidian-kb-chatbot/#enhancing-our-obsidian-chatbot-advanced-rag-techniques-with-langchain>Enhancing Our Obsidian Chatbot: Advanced RAG Techniques with Langchain</a></h2> <p>In our <a href=https://medium.com/@prabhakaran_arivalagan/building-personal-chatbot-with-langchain-ragas-a-journey-of-iteration-and-learning-acbae799124e>previous post</a>, we explored building a chatbot for Obsidian notes using Langchain and basic Retrieval-Augmented Generation (RAG) techniques. Today, I am sharing the significant improvements I've made to enhance the chatbot's performance and functionality. These advancements have transformed our chatbot into a more effective and trustworthy tool for navigating my Obsidian knowledge base.</p> <h3 id=system-architecture-the-blueprint-of-our-enhanced-chatbot><a class=toclink href=2024/08/18/building-obsidian-kb-chatbot/#system-architecture-the-blueprint-of-our-enhanced-chatbot>System Architecture: The Blueprint of Our Enhanced Chatbot</a></h3> <p>Let's start by looking at our updated system architecture:</p> <p>![[Pasted image 20240818163253.png]]</p> <p>This diagram illustrates the flow of our enhanced chatbot, showcasing how each component works together to deliver a seamless user experience. Now, let's dive deeper into each of these components and understand their role in making our chatbot smarter and more efficient.</p> <h3 id=key-improvements-unlocking-new-capabilities><a class=toclink href=2024/08/18/building-obsidian-kb-chatbot/#key-improvements-unlocking-new-capabilities>Key Improvements: Unlocking New Capabilities</a></h3> <p>Our journey of improvement focused on four key areas, each addressing a specific challenge in making our chatbot more responsive and context-aware. Let's explore these enhancements and see how they work together to create a more powerful tool.</p> <h4 id=1-multiquery-retriever-casting-a-wider-net><a class=toclink href=2024/08/18/building-obsidian-kb-chatbot/#1-multiquery-retriever-casting-a-wider-net>1. MultiQuery Retriever: Casting a Wider Net</a></h4> <p>Imagine you're trying to find a specific memory in your vast sea of notes. Sometimes, the way you phrase your question might not perfectly match how you wrote it down. That's where our new MultiQuery Retriever comes in – it's like having a team of creative thinkers helping you remember!</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=bp>self</span><span class=o>.</span><span class=n>multiquery_retriever</span> <span class=o>=</span> <span class=n>CustomMultiQueryRetriever</span><span class=o>.</span><span class=n>from_llm</span><span class=p>(</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>    <span class=bp>self</span><span class=o>.</span><span class=n>retriever</span><span class=p>,</span> <span class=n>llm</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>llm</span><span class=p>,</span> <span class=n>prompt</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>multiquery_retriever_template</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=p>)</span>
</span></code></pre></div> <p>The MultiQuery Retriever is a clever addition that generates multiple variations of your original question. Let's see it in action:</p> <p>Suppose you ask: "What was that interesting AI paper I read last month?"</p> <p>Our MultiQuery Retriever might generate these variations:</p> <ol> <li>"What artificial intelligence research paper did I review in the previous month?"</li> <li>"Can you find any notes about a fascinating AI study from last month?"</li> <li>"List any machine learning papers I found intriguing about 30 days ago."</li> </ol> <p>By creating these diverse phrasings, we significantly increase our chances of finding the relevant information. Maybe you didn't use the term "AI paper" in your notes, but instead wrote "machine learning study." The MultiQuery Retriever helps bridge these verbal gaps, ensuring we don't miss important information due to slight differences in wording.</p> <p>This approach is particularly powerful for:</p> <ul> <li>Complex queries that might be interpreted in multiple ways</li> <li>Recalling information when you're not sure about the exact phrasing you used</li> <li>Uncovering related information that you might not have thought to ask about directly</li> </ul> <p>The result? A much more robust and forgiving search experience that feels almost intuitive, as if the chatbot truly understands the intent behind your questions, not just the literal words you use.</p> <p>Now that we've expanded our search capabilities, let's look at how we've improved the chatbot's understanding of time and context.</p> <h4 id=2-selfquery-retriever-your-personal-time-traveling-assistant><a class=toclink href=2024/08/18/building-obsidian-kb-chatbot/#2-selfquery-retriever-your-personal-time-traveling-assistant>2. SelfQuery Retriever: Your Personal Time-Traveling Assistant</a></h4> <p>While the MultiQuery Retriever helps us find information across different phrasings, the SelfQuery Retriever adds another dimension to our search capabilities: time. Imagine having a super-smart assistant who not only understands your questions but can also navigate through time in your personal knowledge base. That's essentially what our SelfQuery Retriever does – it's like giving our chatbot a time machine!</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=bp>self</span><span class=o>.</span><span class=n>retriever</span> <span class=o>=</span> <span class=n>CustomSelfQueryRetriever</span><span class=o>.</span><span class=n>from_llm</span><span class=p>(</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a>    <span class=n>llm</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>llm</span><span class=p>,</span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a>    <span class=n>vectorstore</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>pinecone_retriever</span><span class=p>,</span>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a>    <span class=n>document_contents</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=vm>__class__</span><span class=o>.</span><span class=n>document_content_description</span><span class=p>,</span>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a>    <span class=n>metadata_field_info</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=vm>__class__</span><span class=o>.</span><span class=n>metadata_field_info</span><span class=p>,</span>
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a><span class=p>)</span>
</span></code></pre></div> <p>The SelfQuery Retriever is a game-changer for handling queries that involve dates. It's particularly useful when you're trying to recall events or information from specific timeframes in your notes. Let's see it in action:</p> <p>Suppose you ask: "What projects was I excited about in the first week of April 2024?"</p> <p>Here's what happens behind the scenes:</p> <ol> <li>The SelfQuery Retriever analyzes your question and understands that you're looking for:<ul> <li>Information about projects</li> <li>Specifically from the first week of April 2024</li> <li>With a positive sentiment ("excited about")</li> </ul> </li> <li> <p>It then translates this into a structured query that might look something like this:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=p>{</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a>  <span class=s2>&quot;query&quot;</span><span class=p>:</span> <span class=s2>&quot;projects excited about&quot;</span><span class=p>,</span>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a>  <span class=s2>&quot;filter&quot;</span><span class=p>:</span> <span class=s2>&quot;and(gte(date, 20240401), lte(date, 20240407))&quot;</span>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a><span class=p>}</span>
</span></code></pre></div> </li> <li> <p>This structured query is used to search your vector database, filtering for documents within that specific date range and then ranking them based on relevance to "projects excited about".</p> </li> </ol> <p>The magic here is that the SelfQuery Retriever can handle a wide range of natural language date queries:</p> <ul> <li>"What did I work on last summer?"</li> <li>"Show me my thoughts on AI from Q1 2024"</li> <li>"Any breakthroughs in my research during the holiday season?"</li> </ul> <p>It understands these temporal expressions and converts them into precise date ranges for searching your notes.</p> <p>The result? A chatbot that feels like it has an intuitive understanding of time, capable of retrieving memories and information from specific periods in your life with remarkable accuracy. It's like having a personal historian who knows exactly when and where to look in your vast archive of experiences.</p> <p>This capability is particularly powerful for:</p> <ul> <li>Tracking progress on long-term projects</li> <li>Recalling ideas or insights from specific time periods</li> <li>Understanding how your thoughts or focus areas have evolved over time</li> </ul> <p>With the SelfQuery Retriever, your Obsidian chatbot doesn't just search your notes – it understands the temporal context of your knowledge, making it an invaluable tool for reflection, planning, and personal growth.</p> <p>But how does the chatbot know when each note was created? Let's explore how we've added this crucial information to our system.</p> <h4 id=3-adding-date-metadata-timestamping-your-thoughts><a class=toclink href=2024/08/18/building-obsidian-kb-chatbot/#3-adding-date-metadata-timestamping-your-thoughts>3. Adding Date Metadata: Timestamping Your Thoughts</a></h4> <p>To support date-based queries and make the SelfQuery Retriever truly effective, we needed a way to associate each note with its creation date. This is where date metadata comes into play. I’ve implemented a system to extract the date from each note's filename and add it as metadata during the indexing process:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=k>def</span><span class=w> </span><span class=nf>extract_date_from_filename</span><span class=p>(</span><span class=n>filename</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>int</span><span class=p>]:</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a>    <span class=n>match</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>match</span><span class=p>(</span><span class=sa>r</span><span class=s2>&quot;(</span><span class=se>\\</span><span class=s2>d</span><span class=si>{4}</span><span class=s2>-</span><span class=se>\\</span><span class=s2>d</span><span class=si>{2}</span><span class=s2>-</span><span class=se>\\</span><span class=s2>d</span><span class=si>{2}</span><span class=s2>)&quot;</span><span class=p>,</span> <span class=n>filename</span><span class=p>)</span>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a>    <span class=k>if</span> <span class=n>match</span><span class=p>:</span>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a>        <span class=n>date_str</span> <span class=o>=</span> <span class=n>match</span><span class=o>.</span><span class=n>group</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a>        <span class=k>try</span><span class=p>:</span>
</span><span id=__span-3-6><a id=__codelineno-3-6 name=__codelineno-3-6 href=#__codelineno-3-6></a>            <span class=n>date_obj</span> <span class=o>=</span> <span class=n>datetime</span><span class=o>.</span><span class=n>strptime</span><span class=p>(</span><span class=n>date_str</span><span class=p>,</span> <span class=n>DATE_FORMAT</span><span class=p>)</span>
</span><span id=__span-3-7><a id=__codelineno-3-7 name=__codelineno-3-7 href=#__codelineno-3-7></a>            <span class=k>return</span> <span class=nb>int</span><span class=p>(</span><span class=n>date_obj</span><span class=o>.</span><span class=n>strftime</span><span class=p>(</span><span class=s2>&quot;%Y%m</span><span class=si>%d</span><span class=s2>&quot;</span><span class=p>))</span>
</span><span id=__span-3-8><a id=__codelineno-3-8 name=__codelineno-3-8 href=#__codelineno-3-8></a>        <span class=k>except</span> <span class=ne>ValueError</span><span class=p>:</span>
</span><span id=__span-3-9><a id=__codelineno-3-9 name=__codelineno-3-9 href=#__codelineno-3-9></a>            <span class=k>return</span> <span class=kc>None</span>
</span><span id=__span-3-10><a id=__codelineno-3-10 name=__codelineno-3-10 href=#__codelineno-3-10></a>    <span class=k>return</span> <span class=kc>None</span>
</span><span id=__span-3-11><a id=__codelineno-3-11 name=__codelineno-3-11 href=#__codelineno-3-11></a>
</span><span id=__span-3-12><a id=__codelineno-3-12 name=__codelineno-3-12 href=#__codelineno-3-12></a><span class=c1># In the indexing process</span>
</span><span id=__span-3-13><a id=__codelineno-3-13 name=__codelineno-3-13 href=#__codelineno-3-13></a><span class=n>document</span><span class=o>.</span><span class=n>metadata</span><span class=p>[</span><span class=s2>&quot;date&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>extract_date_from_filename</span><span class=p>(</span><span class=n>file</span><span class=p>)</span>
</span></code></pre></div> <p>This metadata allows our SelfQuery Retriever to efficiently filter documents based on date ranges or specific dates mentioned in user queries. It's like giving each of your notes a timestamp, allowing the chatbot to organize and retrieve them chronologically when needed.</p> <p>With our chatbot now able to understand both the content and the temporal context of your notes, we've added one more crucial element to make it even more helpful: the ability to remember and use information from your conversation.</p> <h4 id=4-enhancing-multiquery-retriever-with-chat-history-context-aware-question-generation><a class=toclink href=2024/08/18/building-obsidian-kb-chatbot/#4-enhancing-multiquery-retriever-with-chat-history-context-aware-question-generation>4. Enhancing MultiQuery Retriever with Chat History: Context-Aware Question Generation</a></h4> <p>In our previous iteration, we already used chat history to provide context for our LLM's responses. However, we've now taken this a step further by incorporating chat history into our MultiQuery Retriever. This enhancement significantly improves the chatbot's ability to understand and respond to context-dependent queries, especially in ongoing conversations.</p> <p>Let's see how this works in practice:</p> <p>Imagine you're having a conversation with your chatbot about your work projects:</p> <p>You: "What projects did I work on March 1?" Chatbot: [Provides a response about your March 1 projects]</p> <p>You: "How about March 2?"</p> <p>Without context, the MultiQuery Retriever might generate variations like:</p> <ol> <li>"What happened on March 2?"</li> <li>"Events on March 2"</li> <li>"March 2 activities"</li> </ol> <p>These queries, while related to the date, miss the crucial context about projects.</p> <p>However, with our chat history-aware MultiQuery Retriever, it might generate variations like:</p> <ol> <li>"What projects did I work on March 2?"</li> <li>"Project activities on March 2"</li> <li>"March 2 project updates"</li> </ol> <p>These variations are much more likely to retrieve relevant information about your projects on March 2, maintaining the context of your conversation.</p> <p>This improvement is crucial for maintaining coherent, context-aware conversations. Without it, the MultiQuery Retriever could sometimes generate less useful variations, particularly in multi-turn interactions where the context from previous messages is essential.</p> <p>By making the MultiQuery Retriever aware of chat history, we've significantly enhanced its ability to generate relevant query variations. This leads to more accurate document retrieval and, ultimately, more contextually appropriate responses from the chatbot.</p> <p>This enhancement truly brings together the power of our previous improvements. The MultiQuery Retriever now not only casts a wider net with multiple phrasings but does so with an understanding of the conversation's context. Combined with our SelfQuery Retriever's ability to handle temporal queries and our robust date metadata, we now have a chatbot that can navigate your personal knowledge base with remarkable context awareness and temporal understanding.</p> <h3 id=custom-implementations-tailoring-the-tools-to-our-needs><a class=toclink href=2024/08/18/building-obsidian-kb-chatbot/#custom-implementations-tailoring-the-tools-to-our-needs>Custom Implementations: Tailoring the Tools to Our Needs</a></h3> <p>To achieve these enhancements, we created several custom classes, each designed to extend the capabilities of Langchain's base components. Let's take a closer look at two key custom implementations:</p> <ol> <li>CustomMultiQueryRetriever: This class extends the base MultiQueryRetriever to incorporate chat history in query generation.</li> <li>CustomSelfQueryRetriever: We customized the SelfQuery Retriever to work seamlessly with our Pinecone vector store and handle date-based queries effectively.</li> </ol> <p>Here's a snippet from our CustomMultiQueryRetriever to give you a taste of how we've tailored these components:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=k>class</span><span class=w> </span><span class=nc>CustomMultiQueryRetriever</span><span class=p>(</span><span class=n>MultiQueryRetriever</span><span class=p>):</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a>    <span class=k>def</span><span class=w> </span><span class=nf>_get_relevant_documents</span><span class=p>(</span>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a>        <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a>        <span class=n>query</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span>
</span><span id=__span-4-5><a id=__codelineno-4-5 name=__codelineno-4-5 href=#__codelineno-4-5></a>        <span class=n>history</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span>
</span><span id=__span-4-6><a id=__codelineno-4-6 name=__codelineno-4-6 href=#__codelineno-4-6></a>        <span class=o>*</span><span class=p>,</span>
</span><span id=__span-4-7><a id=__codelineno-4-7 name=__codelineno-4-7 href=#__codelineno-4-7></a>        <span class=n>run_manager</span><span class=p>:</span> <span class=n>CallbackManagerForRetrieverRun</span><span class=p>,</span>
</span><span id=__span-4-8><a id=__codelineno-4-8 name=__codelineno-4-8 href=#__codelineno-4-8></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>List</span><span class=p>[</span><span class=n>Document</span><span class=p>]:</span>
</span><span id=__span-4-9><a id=__codelineno-4-9 name=__codelineno-4-9 href=#__codelineno-4-9></a>        <span class=n>queries</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>generate_queries</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>history</span><span class=p>,</span> <span class=n>run_manager</span><span class=p>)</span>
</span><span id=__span-4-10><a id=__codelineno-4-10 name=__codelineno-4-10 href=#__codelineno-4-10></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>include_original</span><span class=p>:</span>
</span><span id=__span-4-11><a id=__codelineno-4-11 name=__codelineno-4-11 href=#__codelineno-4-11></a>            <span class=n>queries</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>query</span><span class=p>)</span>
</span><span id=__span-4-12><a id=__codelineno-4-12 name=__codelineno-4-12 href=#__codelineno-4-12></a>        <span class=n>documents</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>retrieve_documents</span><span class=p>(</span><span class=n>queries</span><span class=p>,</span> <span class=n>run_manager</span><span class=p>)</span>
</span><span id=__span-4-13><a id=__codelineno-4-13 name=__codelineno-4-13 href=#__codelineno-4-13></a>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>unique_union</span><span class=p>(</span><span class=n>documents</span><span class=p>)</span>
</span><span id=__span-4-14><a id=__codelineno-4-14 name=__codelineno-4-14 href=#__codelineno-4-14></a>
</span><span id=__span-4-15><a id=__codelineno-4-15 name=__codelineno-4-15 href=#__codelineno-4-15></a>    <span class=k>def</span><span class=w> </span><span class=nf>generate_queries</span><span class=p>(</span>
</span><span id=__span-4-16><a id=__codelineno-4-16 name=__codelineno-4-16 href=#__codelineno-4-16></a>        <span class=bp>self</span><span class=p>,</span> <span class=n>question</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>history</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>run_manager</span><span class=p>:</span> <span class=n>CallbackManagerForRetrieverRun</span>
</span><span id=__span-4-17><a id=__codelineno-4-17 name=__codelineno-4-17 href=#__codelineno-4-17></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>List</span><span class=p>[</span><span class=nb>str</span><span class=p>]:</span>
</span><span id=__span-4-18><a id=__codelineno-4-18 name=__codelineno-4-18 href=#__codelineno-4-18></a>        <span class=n>response</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>llm_chain</span><span class=o>.</span><span class=n>invoke</span><span class=p>(</span>
</span><span id=__span-4-19><a id=__codelineno-4-19 name=__codelineno-4-19 href=#__codelineno-4-19></a>            <span class=p>{</span><span class=s2>&quot;question&quot;</span><span class=p>:</span> <span class=n>question</span><span class=p>,</span> <span class=s2>&quot;history&quot;</span><span class=p>:</span> <span class=n>history</span><span class=p>},</span>
</span><span id=__span-4-20><a id=__codelineno-4-20 name=__codelineno-4-20 href=#__codelineno-4-20></a>            <span class=n>config</span><span class=o>=</span><span class=p>{</span><span class=s2>&quot;callbacks&quot;</span><span class=p>:</span> <span class=n>run_manager</span><span class=o>.</span><span class=n>get_child</span><span class=p>()},</span>
</span><span id=__span-4-21><a id=__codelineno-4-21 name=__codelineno-4-21 href=#__codelineno-4-21></a>        <span class=p>)</span>
</span><span id=__span-4-22><a id=__codelineno-4-22 name=__codelineno-4-22 href=#__codelineno-4-22></a>        <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>llm_chain</span><span class=p>,</span> <span class=n>LLMChain</span><span class=p>):</span>
</span><span id=__span-4-23><a id=__codelineno-4-23 name=__codelineno-4-23 href=#__codelineno-4-23></a>            <span class=n>lines</span> <span class=o>=</span> <span class=n>response</span><span class=p>[</span><span class=s2>&quot;text&quot;</span><span class=p>]</span>
</span><span id=__span-4-24><a id=__codelineno-4-24 name=__codelineno-4-24 href=#__codelineno-4-24></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-4-25><a id=__codelineno-4-25 name=__codelineno-4-25 href=#__codelineno-4-25></a>            <span class=n>lines</span> <span class=o>=</span> <span class=n>response</span>
</span><span id=__span-4-26><a id=__codelineno-4-26 name=__codelineno-4-26 href=#__codelineno-4-26></a>        <span class=k>return</span> <span class=n>lines</span>
</span></code></pre></div> <p>These custom implementations allow us to tailor the retrieval process to our specific needs, improving the overall performance and relevance of the chatbot's responses.</p> <p>While these enhancements have significantly improved our chatbot, the journey wasn't without its challenges. Let's reflect on some of the hurdles we faced and the lessons we learned along the way.</p> <h3 id=challenges-and-learnings-navigating-the-complexities-of-langchain><a class=toclink href=2024/08/18/building-obsidian-kb-chatbot/#challenges-and-learnings-navigating-the-complexities-of-langchain>Challenges and Learnings: Navigating the Complexities of Langchain</a></h3> <p>While Langchain provides a powerful framework for building RAG systems, we found that its complexity can sometimes be challenging. Digging into different parts of the codebase to understand and modify behavior required significant effort. However, this process also provided valuable insights into the inner workings of RAG systems and allowed us to create a more tailored solution for our Obsidian chatbot.</p> <p>Some key learnings from this process include:</p> <ul> <li>The importance of thoroughly understanding each component before attempting to customize it</li> <li>The value of incremental improvements and testing each change individually</li> <li>The need for patience when working with complex, interconnected systems</li> </ul> <p>These challenges, while sometimes frustrating, ultimately led to a deeper understanding of RAG systems and a more robust final product.</p> <p>Now that we've enhanced our chatbot with these powerful features, let's explore some of the exciting ways it can be used.</p> <h3 id=use-cases-and-examples-putting-our-enhanced-chatbot-to-work><a class=toclink href=2024/08/18/building-obsidian-kb-chatbot/#use-cases-and-examples-putting-our-enhanced-chatbot-to-work>Use Cases and Examples: Putting Our Enhanced Chatbot to Work</a></h3> <p>With these improvements, our Obsidian chatbot is now capable of handling a wider range of queries with improved accuracy. Here are some example use cases that showcase its new capabilities:</p> <ol> <li>Date-specific queries: "What projects was I working on in the first week of March 2024?"</li> <li>Context-aware follow-ups: "Tell me more about the meeting I had last Tuesday."</li> <li>Complex information retrieval: "Summarize my progress on Project X over the last month."</li> </ol> <p>These examples demonstrate the chatbot's ability to understand temporal context, maintain conversation history, and provide more relevant responses. It's not just a search tool anymore – it's becoming a true digital assistant that can help you navigate and make sense of your personal knowledge base.</p> <p>As exciting as these improvements are, we're not stopping here. Let's take a quick look at what's on the horizon for our Obsidian chatbot.</p> <h3 id=future-plans-the-road-ahead><a class=toclink href=2024/08/18/building-obsidian-kb-chatbot/#future-plans-the-road-ahead>Future Plans: The Road Ahead</a></h3> <p>While we've made significant strides in improving our chatbot, there's always room for further enhancements. One exciting avenue we're exploring is the integration of open-source LLMs to make the system more privacy-focused and self-contained. This could potentially allow users to run the entire system locally, ensuring complete privacy of their personal notes and queries.</p> <h3 id=conclusion-a-smarter-more-intuitive-chatbot-for-your-personal-knowledge-base><a class=toclink href=2024/08/18/building-obsidian-kb-chatbot/#conclusion-a-smarter-more-intuitive-chatbot-for-your-personal-knowledge-base>Conclusion: A Smarter, More Intuitive Chatbot for Your Personal Knowledge Base</a></h3> <p>By implementing advanced RAG techniques such as MultiQuery Retriever, SelfQuery Retriever, and incorporating chat history, we've significantly enhanced our Obsidian chatbot's capabilities. These improvements allow for more accurate and contextually relevant responses, especially for date-based queries and complex information retrieval tasks.</p> <p>Building this enhanced chatbot has been a journey of continuous learning and iteration. We've tackled challenges, discovered new possibilities, and created a tool that we hope will make navigating personal knowledge bases easier and more intuitive.</p> <p>We hope that sharing our experience will inspire and help others in the community who are working on similar projects. Whether you're looking to build your own chatbot or simply interested in the possibilities of AI-assisted knowledge management, we hope this post has provided valuable insights.</p> <p>You can find the final code in this <a href=https://github.com/prabha-git/obsidian_kb>GitHub repo</a></p> <p>If you have any feedback or simply want to connect, please hit me up on <a href=https://www.linkedin.com/in/prabha-arivalagan/ >LinkedIn</a> or <a href=https://twitter.com/prabhatweet>@prabha-tweet</a></p> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=https://avatars.githubusercontent.com/u/3776681 alt="Prabha Arivalagan"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2024-04-29 00:00:00+00:00">2024/04/29</time></li> <li class=md-meta__item> 6 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=building-an-obsidian-knowledge-base-chatbot-a-journey-of-iteration-and-learning><a href=2024/04/29/building-obsidian-kb-chatbot/ class=toclink>Building an Obsidian Knowledge base Chatbot: A Journey of Iteration and Learning</a></h2> <p>As an avid Obsidian user, I've always been fascinated by the potential of leveraging my daily notes as a personal knowledge base. Obsidian has become my go-to tool for taking notes, thanks to its simplicity and the wide range of customization options available through community plugins. With the notes and calendar plugins enabled, I can easily capture my daily thoughts and keep track of the projects I'm working on. But what if I could take this a step further and use these notes as the foundation for a powerful chatbot?</p> <p>Imagine having a personal assistant that could answer questions like:</p> <ol> <li>"What was that fascinating blog post I read last week?"</li> <li>"Which projects was I working on back in February 2024?"</li> <li>"Could you give me a quick summary of my activities from last week?"</li> </ol> <p>Excited by the possibilities, I embarked on a journey to build a chatbot that could do just that. In this blog post, I'll share my experience of building this chat app from scratch, including the challenges I faced, the decisions I had to make, and the lessons I learned along the way. You can find the final code in this <a href=https://github.com/prabha-git/obsidian_kb>GitHub repo</a></p> <h3 id=iteration-1-laying-the-groundwork><a class=toclink href=2024/04/29/building-obsidian-kb-chatbot/#iteration-1-laying-the-groundwork>Iteration 1: Laying the Groundwork</a></h3> <p>To kick things off, I decided to start with a simple Retrieval-Augmented Generation (RAG) system for the app. The stack I chose consisted of:</p> <ul> <li>Pinecone for the Vector DB</li> <li>Streamlit for creating the chat interface</li> <li>Langchain framework for tying everything together</li> <li>OpenAI for the Language Model (LLM) and embeddings</li> </ul> <p>I began by embedding my Obsidian daily notes into a Pinecone Vector database. Since my notes aren't particularly lengthy, I opted to embed each daily note as a separate document. Pinecone's simplicity and quick setup allowed me to focus on building the chatbot's functionality rather than getting bogged down in infrastructure.</p> <p>For the language model, I chose OpenAI's GPT-4, as its advanced reasoning capabilities would simplify the app-building process and reduce the need for extensive preprocessing.</p> <p>The initial chatbot workflow looked like this:</p> <p><img alt src=img/Pasted%20image%2020240429153837.png></p> <p>The first version of the chatbot was decent, but I wanted to find a way to measure its performance and track progress as I iterated. After some research, I discovered the <a href=https://docs.ragas.io/en/latest/index.html>RAGAS framework</a>, which is designed specifically for evaluating retrieval-augmented generation systems. By creating a dataset with question-answer pairs, I could measure metrics like answer correctness, relevancy, context precision, recall, and faithfulness.</p> <p><img alt="Chatbot screenshot" src=img/Pasted%20image%2020240429154925.png></p> <p>I included all the metrics available through the RAGAS library, as I was curious to see how they would be affected by my improvements. You can read more about <a href=https://docs.ragas.io/en/latest/concepts/metrics/index.html>RAGAS metrics here</a>. At this stage, I wasn't sure what to make of the numbers or whether they indicated good or bad performance, but it was a starting point.</p> <table> <thead> <tr> <th>Metric</th> <th>Base Performance</th> </tr> </thead> <tbody> <tr> <td>Answer_correctness</td> <td>0.42</td> </tr> <tr> <td>Answer_relevancy</td> <td>0.39</td> </tr> <tr> <td>Answer_similarity</td> <td>0.84</td> </tr> <tr> <td>Context_entity_recall</td> <td>0.27</td> </tr> <tr> <td>Context_precision</td> <td>0.71</td> </tr> <tr> <td>Context_recall</td> <td>0.43</td> </tr> <tr> <td>Context_relevancy</td> <td>0.01</td> </tr> <tr> <td>Faithfulness</td> <td>0.39</td> </tr> </tbody> </table> <h3 id=iteration-2-refining-the-approach><a class=toclink href=2024/04/29/building-obsidian-kb-chatbot/#iteration-2-refining-the-approach>Iteration 2: Refining the Approach</a></h3> <p>With the evaluation framework in place, I reviewed the examples and runs to identify areas for improvement. One thing that stood out was the presence of Dataview queries in my notes. These queries are used in Obsidian to pull data from various notes, similar to SQL queries. However, they don't execute and provide results when the Markdown file is viewed or accessed outside of Obsidian. I realized that these queries might be introducing noise and not adding much value, so I decided to remove them.</p> <p>After making this change and re-evaluating the chatbot, I was surprised to see that the answer metrics had actually gone down. Digging deeper, I discovered that the vector search wasn't yielding the correct daily notes, even for straightforward queries like "What did I do on March 4, 2024?" On the bright side, context precision had improved since the context no longer contained Dataview queries.</p> <table> <thead> <tr> <th>Metric</th> <th>Base</th> <th>Iteration 2</th> </tr> </thead> <tbody> <tr> <td>Answer_correctness</td> <td>0.42</td> <td>0.34</td> </tr> <tr> <td>Answer_relevancy</td> <td>0.39</td> <td>0.36</td> </tr> <tr> <td>Answer_similarity</td> <td>0.84</td> <td>0.81</td> </tr> <tr> <td>Context_entity_recall</td> <td>0.27</td> <td>0.09</td> </tr> <tr> <td>Context_precision</td> <td>0.71</td> <td>0.87</td> </tr> <tr> <td>Context_recall</td> <td>0.43</td> <td>0.42</td> </tr> <tr> <td>Context_relevancy</td> <td>0.01</td> <td>0.02</td> </tr> <tr> <td>Faithfulness</td> <td>0.39</td> <td>0.69</td> </tr> </tbody> </table> <p>To address the issue with vector search, I made two adjustments: 1. Increased the number of documents returned by the retriever from the default 4 to 20. 2. Switched to using a MultiQuery retriever.</p> <p>The goal was to retrieve a larger set of documents, even if their relevancy scores were low, in the hopes that the reranker model would be able to identify and prioritize the most relevant ones.</p> <p>These changes led to a slight improvement in the answer-related metrics compared to the previous iterations. However, the context-related metrics took a hit due to the increased number of documents being considered. I was willing to accept this trade-off for now, as my notes were well-structured, and I believed a highly capable LLM should be able to extract the necessary information.</p> <table> <thead> <tr> <th>Metric</th> <th>Base</th> <th>Iteration 2</th> <th>Iteration 2.1</th> </tr> </thead> <tbody> <tr> <td>Answer_correctness</td> <td>0.42</td> <td>0.34</td> <td>0.45</td> </tr> <tr> <td>Answer_relevancy</td> <td>0.39</td> <td>0.36</td> <td>0.48</td> </tr> <tr> <td>Answer_similarity</td> <td>0.84</td> <td>0.81</td> <td>0.85</td> </tr> <tr> <td>Context_entity_recall</td> <td>0.27</td> <td>0.09</td> <td>0.15</td> </tr> <tr> <td>Context_precision</td> <td>0.71</td> <td>0.87</td> <td>0.62</td> </tr> <tr> <td>Context_recall</td> <td>0.43</td> <td>0.42</td> <td>0.35</td> </tr> <tr> <td>Context_relevancy</td> <td>0.01</td> <td>0.02</td> <td>0.00</td> </tr> <tr> <td>Faithfulness</td> <td>0.39</td> <td>0.69</td> <td>0.56</td> </tr> </tbody> </table> <h3 id=iteration-3-updating-evaluation-dataset><a class=toclink href=2024/04/29/building-obsidian-kb-chatbot/#iteration-3-updating-evaluation-dataset>Iteration 3: Updating Evaluation dataset</a></h3> <p>As I reviewed the evaluation run, I noticed an interesting pattern. When there were no relevant notes to answer a question, the LLM correctly responded with "I don't know." This matched the ground truth, but the answer correctness was being computed as 0.19 instead of a value closer to 1.</p> <p>To improve the evaluation process, I updated the dataset to include "I don't know" as the expected answer in cases where no relevant information was available. This simple change had a significant impact on the answer metrics, providing a more accurate assessment of the chatbot's performance.</p> <table> <thead> <tr> <th>Metric</th> <th>Base</th> <th>Iteration 2</th> <th>Iteration 2.1</th> <th>Iteration 3</th> </tr> </thead> <tbody> <tr> <td>Answer_correctness</td> <td>0.42</td> <td>0.34</td> <td>0.45</td> <td>0.62</td> </tr> <tr> <td>Answer_relevancy</td> <td>0.39</td> <td>0.36</td> <td>0.48</td> <td>0.60</td> </tr> <tr> <td>Answer_similarity</td> <td>0.84</td> <td>0.81</td> <td>0.85</td> <td>0.89</td> </tr> <tr> <td>Context_entity_recall</td> <td>0.27</td> <td>0.09</td> <td>0.15</td> <td>0.14</td> </tr> <tr> <td>Context_precision</td> <td>0.71</td> <td>0.87</td> <td>0.62</td> <td>0.62</td> </tr> <tr> <td>Context_recall</td> <td>0.43</td> <td>0.42</td> <td>0.35</td> <td>0.37</td> </tr> <tr> <td>Context_relevancy</td> <td>0.01</td> <td>0.02</td> <td>0.00</td> <td>0.00</td> </tr> <tr> <td>Faithfulness</td> <td>0.39</td> <td>0.69</td> <td>0.56</td> <td>0.61</td> </tr> </tbody> </table> <h3 id=the-journey-continues><a class=toclink href=2024/04/29/building-obsidian-kb-chatbot/#the-journey-continues>The Journey Continues...</a></h3> <p>At this point, I have a functional chatbot that serves as a powerful search engine for my personal knowledgebase. While I'm happy with the progress so far, there's still room for improvement. Some ideas for future iterations include:</p> <ul> <li>Implementing document retrieval based on metadata like date, to provide more accurate answers for time-sensitive questions.</li> <li>Exploring the use of open-source LLMs like LLAMA3 to keep my data private and self-contained.</li> </ul> <p>Building this chatbot has been an incredible learning experience, showcasing the power of combining Obsidian, vector databases, and language models. Not only has it given me a valuable tool for accessing my own knowledge, but it has also highlighted the importance of iterative development and continuous evaluation.</p> <p>I hope my journey inspires other Obsidian enthusiasts to explore the possibilities of creating their own personal knowledgebase chatbots. By leveraging our daily notes and harnessing the power of AI, we can unlock new ways to interact with and learn from the information we capture.</p> <p>You can find the final code in this <a href=https://github.com/prabha-git/obsidian_kb>GitHub repo</a></p> <p>If you have any feedback or simply want to connect, please hit me up on <a href=https://www.linkedin.com/in/prabha-arivalagan/ >LinkedIn</a> or <a href=https://twitter.com/prabhatweet>@prabha-tweet</a></p> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=https://avatars.githubusercontent.com/u/3776681 alt="Prabha Arivalagan"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2024-04-19 00:00:00+00:00">2024/04/19</time></li> <li class=md-meta__item> 3 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=quantized-llm-models><a href=2024/04/19/quantized-llm-models/ class=toclink>Quantized LLM Models</a></h2> <p>Large Language Models (LLMs) are known for their vast number of parameters, often reaching billions. For example, open-source models like Llama2 come in sizes of 7B, 13B, and 70B parameters, while Google's Gemma has 2B parameters. Although OpenAI's GPT-4 architecture is not publicly shared, it is speculated to have more than a trillion parameters, with 8 models working together in a mixture of experts approach.</p> <h3 id=understanding-parameters><a class=toclink href=2024/04/19/quantized-llm-models/#understanding-parameters>Understanding Parameters</a></h3> <p>A parameter is a model weight learned during the training phase. The number of parameters can be a rough indicator of a model's capability and complexity. These parameters are used in huge matrix multiplications across each layer until an output is produced.</p> <h4 id=the-problem-with-large-number-of-parameters><a class=toclink href=2024/04/19/quantized-llm-models/#the-problem-with-large-number-of-parameters>The Problem with Large Number of Parameters</a></h4> <p>As LLMs have billions of parameters, loading all the parameters into memory and performing massive matrix multiplications becomes a challenge. Let's consider the math behind this:</p> <p>For a 70B parameter model (like the Llama2-70B model), the default size in which these parameters are stored is 32 bits (4 bytes). To load this model, you would need:</p> <p>70B parameters * 4 bytes = 260 GB of memory</p> <p>This highlights the significant memory requirements for running LLMs.</p> <h3 id=quantization-as-a-solution><a class=toclink href=2024/04/19/quantized-llm-models/#quantization-as-a-solution>Quantization as a Solution</a></h3> <p>Quantization is a technique used to reduce the size of the model by decreasing the precision of parameters and storing them in less memory. For example, representing 32-bit floating-point (FP32) parameters in a 16-bit floating-point (FP16) datatype.</p> <p>In practice, this loss of precision does not significantly degrade the output quality of LLMs but offers substantial performance improvements in terms of efficiency. By quantizing the model, the memory footprint can be reduced, making it more feasible to run LLMs on resource-constrained systems.</p> <p>Quantization allows for a trade-off between model size and performance, enabling the deployment of LLMs in a wider range of applications and devices. It is an essential technique for making LLMs more accessible and efficient while maintaining their impressive capabilities.</p> <p>The table below compares the performance of Google’s 2B Gemma model with 32-bit and 16-bit precision. The quantized 16-bit model is 28% faster with approximately 50% less memory usage.</p> <table> <thead> <tr> <th></th> <th>Gemma FP 32 bit precision</th> <th>Gemma FP16 bit precision</th> </tr> </thead> <tbody> <tr> <td># of Parameters</td> <td>2,506,172,416</td> <td>2,506,172,416</td> </tr> <tr> <td>Memory Size based on # Parameters</td> <td>&gt; 2.5B * 4 Bytes<br>9.33 GB</td> <td>&gt; 2.5B * 2 Bytes<br>4.66 GB</td> </tr> <tr> <td>Memory Footprint</td> <td>9.39 GB</td> <td>4.73 GB</td> </tr> <tr> <td>Average Inference time</td> <td>10.36 seconds</td> <td>7.48 seconds</td> </tr> <tr> <td>Distribution of Inference Time</td> <td><img alt src=img/Pasted%20image%2020240420104243.png></td> <td><img alt src=img/Pasted%20image%2020240420104320.png></td> </tr> </tbody> </table> <h3 id=impact-on-accuracy><a class=toclink href=2024/04/19/quantized-llm-models/#impact-on-accuracy>Impact on Accuracy</a></h3> <p>To assess the impact of quantization on accuracy, I ran the output of both models and computed the similarity score using OpenAI's <code>text-embedding-3-large</code> model. The results showed that the similarity scores between the outputs of the 32-bit and 16-bit models were highly comparable with 0.998 cosine similarity, indicating that quantization does not significantly affect the model's accuracy.</p> <p>In conclusion, quantization is a powerful technique for reducing the memory footprint and improving the efficiency of LLMs while maintaining their performance. By enabling the deployment of LLMs on a wider range of devices and applications, quantization plays a crucial role in making these impressive models more accessible and practical for real-world use cases.</p> <div class="admonition note"> <p class=admonition-title>Note</p> <p><em>Inference time and Accuracy are measured for 100 random question, you can find them in the <a href="https://colab.research.google.com/drive/1eR3h_aV1CXUdG9Yzl29o3VR9Yr6J65JG?usp=sharing">colab notebook</a></em></p> </div> <div class="admonition info"> <p class=admonition-title>Good Resource on this topic</p> <p><a href=https://learn.deeplearning.ai/courses/quantization-fundamentals>DLAI - Quantization Fundamentals</a></p> </div> <p>If you have any feedback or simply want to connect, please hit me up on <a href=https://www.linkedin.com/in/prabha-arivalagan/ >LinkedIn</a> or <a href=https://twitter.com/prabhatweet>@prabha-tweet</a></p> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=https://avatars.githubusercontent.com/u/3776681 alt="Prabha Arivalagan"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2024-03-19 00:00:00+00:00">2024/03/19</time></li> <li class=md-meta__item> 4 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=karpathys-lets-build-gpt-from-scratch><a href=2024/03/19/lets-build-gpt-from-scratch/ class=toclink>Karpathy's let's build GPT from scratch</a></h2> <div class="admonition note"> <p class=admonition-title>Self Note</p> <p>This note is for me to understand the concepts</p> </div> <div class="admonition note"> <p class=admonition-title>Learning Resource</p> <p>Karpathy's tutorial on Youtube <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY&t=2794s">Lets build GPT from scratch</a></p> </div> <h3 id=the-spelled-out-intro-to-neural-networks-and-backpropagation-building-micrograd-youtube><a class=toclink href=2024/03/19/lets-build-gpt-from-scratch/#the-spelled-out-intro-to-neural-networks-and-backpropagation-building-micrograd-youtube><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=1">The spelled-out intro to neural networks and backpropagation: building micrograd - YouTube</a></a></h3> <div class="language-text highlight"><pre><span></span><code>In this video he buils micrograd
</code></pre></div> <h3 id=the-spelled-out-intro-to-language-modeling-building-makemore-youtube><a class=toclink href=2024/03/19/lets-build-gpt-from-scratch/#the-spelled-out-intro-to-language-modeling-building-makemore-youtube><a href="https://youtu.be/PaCmpygFfXo?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">The spelled-out intro to language modeling: building makemore - YouTube</a></a></h3> <div class="language-text highlight"><pre><span></span><code>Building makemore [GitHub - karpathy/makemore: An autoregressive character-level language model for making more things](https://github.com/karpathy/makemore)

Dataset: people names dataset in givernment website
</code></pre></div> <h4 id=iteration-1><a class=toclink href=2024/03/19/lets-build-gpt-from-scratch/#iteration-1>Iteration 1:</a></h4> <div class="language-text highlight"><pre><span></span><code>    Character level language model

    Method: Bigram (Predict next char using previous char)
</code></pre></div> <p><img alt=Pasted%20image%2020250130124540 src=img/Pasted%20image%2020250130124540.png> As seens above, it doesn't give good names. Bigram model is not good for predicting next character.</p> <div class="language-text highlight"><pre><span></span><code>In &quot;bigram&quot; model probabilities become the parameter of bigram language model.
</code></pre></div> <h4 id=quality-evaluation-of-model><a class=toclink href=2024/03/19/lets-build-gpt-from-scratch/#quality-evaluation-of-model>Quality Evaluation of model</a></h4> <p>We will be using [[Negative maximum log likelihood estimate]] , in our problem we will calculate for the entire training set. </p> <div class="language-text highlight"><pre><span></span><code>Log 1 = 0 &amp; Log (very small number ) = -Inf
</code></pre></div> <p>We would estimate Negative Log likelihood as follows </p> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=n>log_likelihood</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a><span class=n>n</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=k>for</span> <span class=n>w</span> <span class=ow>in</span> <span class=n>words</span><span class=p>[:</span><span class=mi>3</span><span class=p>]:</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a>    <span class=n>chs</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;.&#39;</span><span class=p>]</span> <span class=o>+</span> <span class=nb>list</span><span class=p>(</span><span class=n>w</span><span class=p>)</span> <span class=o>+</span> <span class=p>[</span><span class=s1>&#39;.&#39;</span><span class=p>]</span>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a>    <span class=k>for</span> <span class=n>ch1</span><span class=p>,</span> <span class=n>ch2</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>chs</span><span class=p>,</span> <span class=n>chs</span><span class=p>[</span><span class=mi>1</span><span class=p>:]):</span>
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a>        <span class=n>ix1</span> <span class=p>,</span> <span class=n>ix2</span> <span class=o>=</span> <span class=n>stoi</span><span class=p>[</span><span class=n>ch1</span><span class=p>],</span> <span class=n>stoi</span><span class=p>[</span><span class=n>ch2</span><span class=p>]</span>
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a>        <span class=n>prob</span><span class=o>=</span><span class=n>P</span><span class=p>[</span><span class=n>ix1</span><span class=p>,</span> <span class=n>ix2</span><span class=p>]</span> <span class=c1># P is the matrix that holds the probability</span>
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a>        <span class=n>n</span><span class=o>+=</span><span class=mi>1</span>
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a>        <span class=n>log_likelihood</span><span class=o>+=</span><span class=n>torch</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>prob</span><span class=p>)</span>
</span><span id=__span-0-10><a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>ch1</span><span class=si>}{</span><span class=n>ch2</span><span class=si>}</span><span class=s1>: </span><span class=si>{</span><span class=n>prob</span><span class=si>:</span><span class=s1>.4f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span><span id=__span-0-11><a id=__codelineno-0-11 name=__codelineno-0-11 href=#__codelineno-0-11></a>
</span><span id=__span-0-12><a id=__codelineno-0-12 name=__codelineno-0-12 href=#__codelineno-0-12></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>log_likelihood</span><span class=si>=}</span><span class=s1>&#39;</span><span class=p>)</span>
</span><span id=__span-0-13><a id=__codelineno-0-13 name=__codelineno-0-13 href=#__codelineno-0-13></a>
</span><span id=__span-0-14><a id=__codelineno-0-14 name=__codelineno-0-14 href=#__codelineno-0-14></a><span class=c1>#Negative log likelihood give nice property where error (loss function) should be small, i.e zero is good.</span>
</span><span id=__span-0-15><a id=__codelineno-0-15 name=__codelineno-0-15 href=#__codelineno-0-15></a><span class=n>nll</span> <span class=o>=</span> <span class=o>-</span><span class=n>log_likelihood</span>
</span><span id=__span-0-16><a id=__codelineno-0-16 name=__codelineno-0-16 href=#__codelineno-0-16></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>nll</span><span class=si>=}</span><span class=s1>&#39;</span><span class=p>)</span>
</span><span id=__span-0-17><a id=__codelineno-0-17 name=__codelineno-0-17 href=#__codelineno-0-17></a>
</span><span id=__span-0-18><a id=__codelineno-0-18 name=__codelineno-0-18 href=#__codelineno-0-18></a><span class=c1>#Usually people work with average negative log likelihood</span>
</span><span id=__span-0-19><a id=__codelineno-0-19 name=__codelineno-0-19 href=#__codelineno-0-19></a><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>nll</span><span class=o>/</span><span class=n>n</span><span class=si>=}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></code></pre></div> <p>To avoid infinity probability for some predictions, people do model "smoothing" (assigning very small probability to unlikely scenario)</p> <h4 id=iteration-2-bigram-language-model-using-neural-network><a class=toclink href=2024/03/19/lets-build-gpt-from-scratch/#iteration-2-bigram-language-model-using-neural-network>Iteration 2: Bigram Language Model using Neural Network</a></h4> <p>Need to create a dataset for training, i.e input and output char pair. (x and y).</p> <p>One hot encoding needs to be done before feeding into NN</p> <p><code>Log {count} = Logits</code> <code>counts = exp(Logits)</code></p> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=n>xenc</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>one_hot</span><span class=p>(</span><span class=n>xs</span><span class=p>,</span> <span class=n>num_classes</span> <span class=o>=</span> <span class=mi>27</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>100</span><span class=p>):</span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a>    <span class=c1># Forward Pass</span>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a>    <span class=n>logits</span> <span class=o>=</span> <span class=n>xenc</span> <span class=o>@</span> <span class=n>W</span> <span class=c1># Pred log-counts</span>
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a>
</span><span id=__span-1-7><a id=__codelineno-1-7 name=__codelineno-1-7 href=#__codelineno-1-7></a>    <span class=n>counts</span> <span class=o>=</span> <span class=n>logits</span><span class=o>.</span><span class=n>exp</span><span class=p>()</span> <span class=c1># Counts</span>
</span><span id=__span-1-8><a id=__codelineno-1-8 name=__codelineno-1-8 href=#__codelineno-1-8></a>
</span><span id=__span-1-9><a id=__codelineno-1-9 name=__codelineno-1-9 href=#__codelineno-1-9></a>    <span class=n>probs</span> <span class=o>=</span> <span class=n>counts</span>  <span class=o>/</span> <span class=n>counts</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdims</span> <span class=o>=</span> <span class=kc>True</span><span class=p>)</span> 
</span><span id=__span-1-10><a id=__codelineno-1-10 name=__codelineno-1-10 href=#__codelineno-1-10></a>
</span><span id=__span-1-11><a id=__codelineno-1-11 name=__codelineno-1-11 href=#__codelineno-1-11></a>    <span class=n>loss</span> <span class=o>=</span> <span class=o>-</span><span class=n>probs</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>228146</span><span class=p>),</span> <span class=n>ys</span><span class=p>]</span><span class=o>.</span><span class=n>log</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span><span id=__span-1-12><a id=__codelineno-1-12 name=__codelineno-1-12 href=#__codelineno-1-12></a>    <span class=nb>print</span><span class=p>(</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>())</span>
</span><span id=__span-1-13><a id=__codelineno-1-13 name=__codelineno-1-13 href=#__codelineno-1-13></a>
</span><span id=__span-1-14><a id=__codelineno-1-14 name=__codelineno-1-14 href=#__codelineno-1-14></a>    <span class=c1>#Backward pass</span>
</span><span id=__span-1-15><a id=__codelineno-1-15 name=__codelineno-1-15 href=#__codelineno-1-15></a>    <span class=n>W</span><span class=o>.</span><span class=n>grad</span><span class=o>=</span><span class=kc>None</span>
</span><span id=__span-1-16><a id=__codelineno-1-16 name=__codelineno-1-16 href=#__codelineno-1-16></a>    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span><span id=__span-1-17><a id=__codelineno-1-17 name=__codelineno-1-17 href=#__codelineno-1-17></a>
</span><span id=__span-1-18><a id=__codelineno-1-18 name=__codelineno-1-18 href=#__codelineno-1-18></a>    <span class=c1>#Update parameters using the gradient calculated</span>
</span><span id=__span-1-19><a id=__codelineno-1-19 name=__codelineno-1-19 href=#__codelineno-1-19></a>    <span class=n>W</span><span class=o>.</span><span class=n>data</span><span class=o>+=</span> <span class=o>-</span><span class=mi>50</span>  <span class=o>*</span> <span class=n>W</span><span class=o>.</span><span class=n>grad</span> <span class=c1># here 50 is h , initial tried small numbers , like 0.1 but it is decreasing the loss very slowly hence increased to 50</span>
</span></code></pre></div> <h4 id=thoughts-and-comparison-of-above-two-approaches><a class=toclink href=2024/03/19/lets-build-gpt-from-scratch/#thoughts-and-comparison-of-above-two-approaches>Thoughts and comparison of above two approaches</a></h4> <p>In the first approach, we added 1 to the actual count because we don't want to end up in a situation it give <span class=arithmatex>\(-\infty\)</span> for the character pair it didn't see in the trainin dataset. If you add large number then actual frequency is less relevent and we get uniform distribution. It is called smoothing</p> <p>Similarly, gradient based approach has a way to "smoothing". When you keep all values of <code>W</code> to be zero, exp(W) gives all ones and softmax would provide equal probabilities to all outputs. You incentivise this in loss function by using second component like below </p> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=n>loss</span> <span class=o>=</span> <span class=o>-</span><span class=n>probs</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>228146</span><span class=p>),</span> <span class=n>ys</span><span class=p>]</span><span class=o>.</span><span class=n>log</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span> <span class=o>+</span> <span class=p>(</span><span class=mf>0.1</span> <span class=o>*</span> <span class=p>(</span><span class=n>W</span><span class=o>**</span><span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>())</span>
</span></code></pre></div> <p>Second component pushed W to be zero , 0.1 is the strength of Regularization that determines the how much weight we want to give to this regularization component. It is similar to the number of "fake" count you add in the first approach.</p> <p>We took two approaches </p> <p>i) Frequency based model ii) NN based model (using Negative log likelihood to optimize)</p> <p>We ended up with the same model , in the NN based approach the <code>W</code> represents the log probability (same as first approach) , we can exponential the <code>W</code> to get count </p> <h3 id=building-makemore-part-2-mlp-youtube><a class=toclink href=2024/03/19/lets-build-gpt-from-scratch/#building-makemore-part-2-mlp-youtube><a href="https://www.youtube.com/watch?v=TCH_1BHY58I">Building makemore Part 2: MLP - YouTube</a></a></h3> <p>In this class we would build makemore to predict based on last 3 characters.</p> <h5 id=embedding><a class=toclink href=2024/03/19/lets-build-gpt-from-scratch/#embedding>Embedding</a></h5> <p>As a first step, we need to build embedding for the characters, we start with 2 dimensional embedding.</p> <p><img alt=Pasted%20image%2020250130124540 src=img/Pasted%20image%2020250205123847.png></p> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=n>h</span> <span class=o>=</span> <span class=n>emb</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>6</span><span class=p>)</span> <span class=o>@</span> <span class=n>W1</span> <span class=o>+</span> <span class=n>b1</span> <span class=c1># Hiden layer activation</span>
</span></code></pre></div> <p>We index on embedding matrix to get the weight / embeddings for the character. Another way to interpret is one hot encoding. indexing and one hot encoding produce similar result. in this case we think first layer as weight of neural network.</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=n>logits</span> <span class=o>=</span> <span class=n>h</span> <span class=o>@</span> <span class=n>W2</span> <span class=o>+</span> <span class=n>b2</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a><span class=n>counts</span> <span class=o>=</span> <span class=n>logits</span><span class=o>.</span><span class=n>exp</span><span class=p>()</span>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a><span class=n>prob</span> <span class=o>=</span> <span class=n>counts</span><span class=o>/</span><span class=n>counts</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=n>keepdims</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a><span class=n>prob</span><span class=o>.</span><span class=n>shape</span>
</span><span id=__span-4-5><a id=__codelineno-4-5 name=__codelineno-4-5 href=#__codelineno-4-5></a><span class=c1># torch.Size([32, 27])</span>
</span></code></pre></div> <p>In Final layer we get probability distribution for all 27 characters.</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=c1># Negative Log likelihood </span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a>
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a><span class=n>loss</span> <span class=o>=</span> <span class=o>-</span><span class=n>prob</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>32</span><span class=p>),</span> <span class=n>Y</span><span class=p>]</span><span class=o>.</span><span class=n>log</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a><span class=n>loss</span>
</span></code></pre></div> <p>In Practice, we use mini batch for forward or backward pass. it is efficient than optimizing on the entire dataset.</p> <p>it is much efficient to take many steps (iteration) with low confidence in gradient</p> <h5 id=learning-rate><a class=toclink href=2024/03/19/lets-build-gpt-from-scratch/#learning-rate>Learning rate</a></h5> <p>Learning rate is an important hyper , we need to find the reasonable range manually and we can use different techniques to search for the optimal parameter in that range.</p> <h5 id=dataset-split><a class=toclink href=2024/03/19/lets-build-gpt-from-scratch/#dataset-split>Dataset split</a></h5> <p>Important to split dataset into three sets - train split is to find model parameters </p> <ul> <li> <p>dev split is to find hyper parameters</p> </li> <li> <p>test split is to evaluate the model performance finally</p> </li> </ul> <p>we improve the model by increasing the complexity by increasing the parameters. for example hidden layer neurons can be increased.</p> <p>In our case , bottle neck may be the embeddings, we are cramping all the character in just two dimensional space. we can increase embedding dimensions to 10 from 2.</p> <p>Now we get better name sounding words than before ( with just one character in context)</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a>dex.
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a>marial.
</span><span id=__span-6-3><a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a>mekiophity.
</span><span id=__span-6-4><a id=__codelineno-6-4 name=__codelineno-6-4 href=#__codelineno-6-4></a>nevonimitta.
</span><span id=__span-6-5><a id=__codelineno-6-5 name=__codelineno-6-5 href=#__codelineno-6-5></a>nolla.
</span><span id=__span-6-6><a id=__codelineno-6-6 name=__codelineno-6-6 href=#__codelineno-6-6></a>kyman.
</span><span id=__span-6-7><a id=__codelineno-6-7 name=__codelineno-6-7 href=#__codelineno-6-7></a>arreyzyne.
</span><span id=__span-6-8><a id=__codelineno-6-8 name=__codelineno-6-8 href=#__codelineno-6-8></a>javer.
</span><span id=__span-6-9><a id=__codelineno-6-9 name=__codelineno-6-9 href=#__codelineno-6-9></a>gota.
</span><span id=__span-6-10><a id=__codelineno-6-10 name=__codelineno-6-10 href=#__codelineno-6-10></a>mic.
</span><span id=__span-6-11><a id=__codelineno-6-11 name=__codelineno-6-11 href=#__codelineno-6-11></a>jenna.
</span><span id=__span-6-12><a id=__codelineno-6-12 name=__codelineno-6-12 href=#__codelineno-6-12></a>osie.
</span><span id=__span-6-13><a id=__codelineno-6-13 name=__codelineno-6-13 href=#__codelineno-6-13></a>tedo.
</span><span id=__span-6-14><a id=__codelineno-6-14 name=__codelineno-6-14 href=#__codelineno-6-14></a>kaley.
</span><span id=__span-6-15><a id=__codelineno-6-15 name=__codelineno-6-15 href=#__codelineno-6-15></a>mess.
</span><span id=__span-6-16><a id=__codelineno-6-16 name=__codelineno-6-16 href=#__codelineno-6-16></a>suhaiaviyny.
</span><span id=__span-6-17><a id=__codelineno-6-17 name=__codelineno-6-17 href=#__codelineno-6-17></a>fobs.
</span><span id=__span-6-18><a id=__codelineno-6-18 name=__codelineno-6-18 href=#__codelineno-6-18></a>mhiriel.
</span><span id=__span-6-19><a id=__codelineno-6-19 name=__codelineno-6-19 href=#__codelineno-6-19></a>vorreys.
</span><span id=__span-6-20><a id=__codelineno-6-20 name=__codelineno-6-20 href=#__codelineno-6-20></a>dasdro.
</span></code></pre></div> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=https://avatars.githubusercontent.com/u/3776681 alt="Prabha Arivalagan"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2024-03-06 00:00:00+00:00">2024/03/06</time></li> <li class=md-meta__item> 1 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=tokenizer-comparison><a href=2024/03/06/tokenizer-comparison/ class=toclink>Tokenizer comparison</a></h2> <div class="admonition note"> <p class=admonition-title>Self Note</p> <p>This note is for myself to understand the concepts</p> </div> <p>Very good resource <a href="https://www.youtube.com/watch?v=zduSFxRajkE&t=3618s">Karpathy's Tokenizer Video</a></p> <p>It is always clear that LLMs use different tokenizer , i want to test it. </p> <p>I have downloaded from <a href=https://www.gutenberg.org/cache/epub/100/pg100.txt>gutenberg.org/cache/epub/100/pg100.txt</a></p> <p>comparison of <a href="https://colab.research.google.com/drive/1jYl4aW65Ko1e8QBca2b5hJvxrASJ-dHf#scrollTo=pOHNGlhTv3Zy">My Colab Notebook</a></p> <p><img alt="token vs characters" src=img/Pasted_image_20240306162048.png></p> </div> </article> <nav class=md-pagination> </nav> </div> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2024 Prabha Arivalagan </div> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"annotate": null, "base": "..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.code.select", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.expand", "navigation.indexes", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.prune", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.7a47a382.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.e71a0d61.min.js></script> <script src=../javascripts/mathjax.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>